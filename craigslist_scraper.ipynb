{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, datetime\n",
    "import time \n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7359b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432, # your server hostname\n",
    "    database=\"cars\",     # your database name\n",
    "    user=\"postgres\",   # your database username\n",
    "    password=\"p33Gritz!!\" # your database password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea727ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['ABS',\n",
    " 'Trim2',\n",
    " 'ESC',\n",
    " 'SteeringLocation',\n",
    " 'BatteryInfo',\n",
    " 'DaytimeRunningLight',\n",
    " 'PedestrianAutomaticEmergencyBraking',\n",
    " 'TransmissionStyle',\n",
    " 'WheelBaseType',\n",
    " 'Trim',\n",
    " 'ChargerLevel',\n",
    " 'AutomaticPedestrianAlertingSound',\n",
    " 'TractionControl',\n",
    " 'AirBagLocFront',\n",
    " 'Pretensioner',\n",
    " 'TransmissionSpeeds',\n",
    " 'AdaptiveDrivingBeam',\n",
    " 'Model',\n",
    " 'BlindSpotMon',\n",
    " 'EntertainmentSystem',\n",
    " 'BodyCabType',\n",
    " 'FuelTypeSecondary',\n",
    " 'LaneDepartureWarning',\n",
    " 'TPMS',\n",
    " 'Seats',\n",
    " 'FuelInjectionType',\n",
    " 'EDR',\n",
    " 'LowerBeamHeadlampLightSource',\n",
    " 'ParkAssist',\n",
    " 'AirBagLocCurtain',\n",
    " 'RearAutomaticEmergencyBraking',\n",
    " 'RearCrossTrafficAlert',\n",
    " 'SemiautomaticHeadlampBeamSwitching',\n",
    " 'CIB',\n",
    " 'AirBagLocSide',\n",
    " 'BrakeSystemDesc',\n",
    " 'KeylessIgnition',\n",
    " 'EngineConfiguration',\n",
    " 'AirBagLocKnee',\n",
    " 'RearVisibilitySystem',\n",
    " 'VehicleType',\n",
    " 'AdaptiveCruiseControl',\n",
    " 'AirBagLocSeatCushion',\n",
    " 'BlindSpotIntervention',\n",
    " 'ForwardCollisionWarning',\n",
    " 'SeatRows',\n",
    " 'BatteryType',\n",
    " 'LaneKeepSystem',\n",
    " 'GVWR',\n",
    " 'ElectrificationLevel',\n",
    " 'DynamicBrakeSupport',\n",
    " 'LaneCenteringAssistance',\n",
    " 'BedType',\n",
    " 'BrakeSystemType',\n",
    " 'Series2',\n",
    " 'CoolingType',\n",
    " 'Doors',\n",
    " 'EngineCylinders',\n",
    " 'CAN_AACN',\n",
    " 'Turbo',\n",
    " 'BodyClass',\n",
    " 'DriveType',\n",
    " 'ValveTrainDesign',\n",
    " 'FuelTypePrimary',\n",
    " 'Make',\n",
    " 'AutoReverseSystem',\n",
    " 'EVDriveUnit',\n",
    " 'Series',\n",
    " 'SeatBeltsAll',\n",
    " 'PlantCity',\n",
    " 'PlantCountry',\n",
    " 'PlantState',\n",
    " 'Note',\n",
    " 'OtherEngineInfo',\n",
    " 'GVWR_to',\n",
    " 'EngineModel',\n",
    " 'DestinationMarket',\n",
    " 'ActiveSafetySysNote',\n",
    " 'state',\n",
    " 'region',\n",
    " 'condition',\n",
    "'paint_color']\n",
    "\n",
    "nums =  ['ModelYear',\n",
    " 'WheelSizeRear',\n",
    " 'BasePrice',\n",
    " 'WheelSizeFront',\n",
    " 'CurbWeightLB',\n",
    " 'WheelBaseShort',\n",
    " 'WheelBaseLong',\n",
    " 'BatteryPacks',\n",
    " 'SAEAutomationLevel',\n",
    " 'odometer',\n",
    " 'EngineHP',\n",
    " 'TopSpeedMPH',\n",
    " 'TrackWidth',\n",
    " 'ChargerPowerKW',\n",
    " 'EngineKW',\n",
    " 'EngineHP_to',\n",
    " 'BatteryKWh',\n",
    " 'BedLengthIN',\n",
    " 'BatteryV',\n",
    " 'DisplacementCC',\n",
    " 'Wheels',\n",
    " 'Windows',\n",
    " 'days_since',\n",
    " 'state_income']\n",
    "\n",
    "def model_prep(df2):\n",
    "    df2[cats] = df2[cats].astype(str)\n",
    "    df2[nums] = df2[nums].astype('float64')\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "\n",
    "reg_ref = pd.read_sql(''' SELECT * FROM region_reference;''', conn,index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#car_data = pd.read_sql(''' SELECT * FROM car_data;''', conn,index_col='id')\n",
    "#car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = pd.read_csv('df_copy.csv',index_col=[0], dtype={'EngineCylinders': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432, # your server hostname\n",
    "    database=\"cars\",     # your database name\n",
    "    user=\"postgres\",   # your database username\n",
    "    password=\"p33Gritz!!\" # your database password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql+psycopg2://postgres:p33Gritz!!@localhost:5432/cars')\n",
    "new_data.to_sql('car_data_latest', engine, index=True, index_label='id', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure\n",
    "\n",
    "new_vins = load up 'vins_accepted' + datestr\n",
    "Drop duplicates on ['VIN']\n",
    "\n",
    "new_listings = load up 'listings_accepted' + datestr\n",
    "drop duplicates on ['VIN']\n",
    "\n",
    "new_links = pd.read_sql('links_accepted' + datestr, engine)\n",
    "\n",
    "links_listing_merge = pd.merge(new_listings, new_links, on='link', how='left')\n",
    "\n",
    "f_df = model_prep(posting_date(pd.merge(new_vins.drop_duplicates(subset=['VIN']), links_listing_merge.drop_duplicates(subset=['VIN']), on='VIN', how='left')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5d3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e141bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_sql('good_vin_2024_10_28', engine)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.drop_duplicates(subset=['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[new_data.VIN.isin(new_listing_df.VIN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_listing_df = pd.read_sql('new_listings_2024_10_28', engine)\n",
    "new_listing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12818f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = new_listing_df.drop_duplicates(subset=['VIN'])\n",
    "list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64896367",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_listing_merge = pd.merge(list_df, new_links_df, on='link', how='left')\n",
    "links_listing_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_listing_merge.link.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc61d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = posting_date(pd.merge(new_data.drop_duplicates(subset=['VIN']), links_listing_merge.drop_duplicates(subset=['VIN']), on='VIN', how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df.days_since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links_df = pd.read_sql('new_links_2024_10_28', engine)\n",
    "new_links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaecf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_data['link'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d99cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512bb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d895d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_table_from_dataframe(df, final_table_name, dbname='cars', user='postgres', password='p33Gritz!!', host='localhost', port='5432', use_sqlalchemy=True):\n",
    "\n",
    "    if use_sqlalchemy:\n",
    "        # Using SQLAlchemy\n",
    "        engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "        with engine.connect() as connection:\n",
    "            # Create the table in the SQL database\n",
    "            df.to_sql(final_table_name, con=connection, if_exists='replace', index=False)\n",
    "            print(f\"Table '{final_table_name}' created successfully using SQLAlchemy.\")\n",
    "    else:\n",
    "        # Using psycopg2\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=dbname,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        try:\n",
    "            # Create the table using raw SQL\n",
    "            # First, create the SQL command based on DataFrame columns\n",
    "            columns = \", \".join([f\"{col} TEXT\" for col in df.columns])  # Use TEXT for simplicity, adjust as needed\n",
    "            create_table_query = f\"CREATE TABLE {final_table_name} ({columns});\"\n",
    "            cur.execute(create_table_query)\n",
    "            conn.commit()\n",
    "\n",
    "            # Insert DataFrame data into the new table\n",
    "            for index, row in df.iterrows():\n",
    "                insert_query = f\"INSERT INTO {final_table_name} VALUES ({', '.join(['%s'] * len(row))});\"\n",
    "                cur.execute(insert_query, tuple(row))\n",
    "\n",
    "            conn.commit()\n",
    "            print(f\"Table '{final_table_name}' created successfully using psycopg2.\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()  # Rollback in case of error\n",
    "            print(f\"Error occurred: {e}\")\n",
    "        finally:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "'''\n",
    "\n",
    "def df_to_table(df, final_table_name, engine, exist = 'replace'):\n",
    "    with engine.connect() as connection:\n",
    "        df.to_sql(final_table_name, con=connection, if_exists=exist, index=False)\n",
    "        print(f\"Table '{final_table_name}' created successfully using SQLAlchemy.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb91aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_links(backup_df, scraped_links, engine, datestr, exist='replace'):\n",
    "    \n",
    "    #link_df = pd.read_sql(link_table, engine)\n",
    "    #backup_df = pd.read_sql(backup_table, engine)\n",
    "    \n",
    "    repeat_mask = scraped_links.link.isin(backup_df.link)\n",
    "    \n",
    "    need_these = clean_mask_price(scraped_links[~repeat_mask], 4000, 110000) \n",
    "    repeats = scraped_links[repeat_mask]\n",
    "\n",
    "    df_to_table(need_these, 'new_links' + datestr, engine, exist)\n",
    "    print('new_links' + datestr + ' created')\n",
    "    df_to_table(repeats, 'repeat_links' + datestr, engine, exist)\n",
    "    print('repeat_links' + datestr + ' created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask_price(df, mini, maxi):\n",
    "    if df['price'].dtype == 'object':\n",
    "        df['price'] = df['price'].str.replace(',', '').str.replace('$', '').astype(float)\n",
    "    return df[(df['price'] > mini) & (df['price'] < maxi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_one(main_data, datestr, reg_ref = 'region_reference', \n",
    "            dbname='cars', user='postgres', password='p33Gritz!!', host='localhost', port='5432'):\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "    \n",
    "    # backup table : main_data + datestr\n",
    "    old_df = backup_table(main_data, datestr, dbname=dbname, user=user, password=password, host=host, port=port, use_sqlalchemy=True)\n",
    "    \n",
    "    # creates SQL tables\n",
    "    # 'repeat_links_' + datestr\n",
    "    # 'new_links_' + datestr\n",
    "    \n",
    "    scraped_links = pd.concat(scrape_regions(reg_ref))\n",
    "    sort_links(old_df, scraped_links, engine, datestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_regions(df):\n",
    "    dfls = []\n",
    "\n",
    "    for region_url in df.region_url.unique():\n",
    "        link = region_url + '/search/cta?auto_title_status=1&bundleDuplicates=1&query=vin#search=1~gallery~0~0'\n",
    "        tdf = df_from_link(link)\n",
    "        tdf['region_url'] = region_url\n",
    "        dfls.append(tdf)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    return dfls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_everything(main_data, datestr, reg_ref = 'region_reference', \n",
    "            dbname='cars', user='postgres', password='p33Gritz!!', host='localhost', port='5432'):\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "    old_df = backup_table(main_data, datestr, dbname=dbname, user=user, password=password, host=host, port=port, use_sqlalchemy=True)\n",
    "    \n",
    "    # 'repeat_links_' + datestr\n",
    "    # 'new_links_' + datestr\n",
    "    \n",
    "    scraped_links = pd.concat(scrape_regions(reg_ref))\n",
    "    sort_links(old_df, scraped_links, engine, datestr)\n",
    "    \n",
    "    # creates 'good_vin_', 'bad_vin_', 'new_listings_', 'reject_listings_'\n",
    "    scrape_listings('new_links' + datestr, 'new_listings' + datestr, 'reject_listings' + datestr)\n",
    "    \n",
    "    good_vin_df = pd.read_sql('good_vin_' + datestr, engine)\n",
    "    new_listing_df = pd.read_sql('new_listings_' + datestr, engine)\n",
    "    reg_ref_df = pd.read_sql(region_reference, engine)\n",
    "    new_link_df = pd.read_sql('new_links' + datestr, engine)\n",
    "    \n",
    "    vin_plus = pd.merge(good_vin_df, new_listing_df)\n",
    "    f_df = model_prep(posting_date(pd.merge(vin_plus, need_these, on='link',how='left')))\n",
    "    \n",
    "    df = pd.concat([model_prep(old_df), f_df])\n",
    "    \n",
    "    df.to_sql(main_data, engine)\n",
    "    \n",
    "    cbm = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=10, silent=True)\n",
    "    mod_name = 'cb_model_' + datestr + '.cbm'\n",
    "    cbm.fit(df[cats+nums], df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('new_links_2024_10_28', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1eeebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_links(modern_data, scraped_links, engine, '_2024_10_28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e727684",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_df = posting_date(pd.merge(new_data, links_listing_merge.drop_duplicates(subset=['VIN']), on='VIN', how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ca385",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in old_data.columns.tolist() if x not in f_df.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.read_sql('car_data', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a89192",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d = old_data.drop_duplicates(subset=['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d.link.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d[n_d.link.notna()].to_sql('modern_car_data', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed12127",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_data.link.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links_df[~new_links_df.link.isin(modern_data.link)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4712d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_data = n_d[n_d.link.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data2 = model_prep(modern_data)\n",
    "df1 = model_prep(f_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_m = CatBoostRegressor()\n",
    "\n",
    "cb_m.load_model('cb_model_latest.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eac1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_retrain_update(old_data, good_vin, new_listings, new_links, datestr, cats=cats, nums=nums):\n",
    "    \n",
    "    vin_plus = pd.merge(good_vin, new_listings)\n",
    "    f_df = model_prep(posting_date(pd.merge(vin_plus, new_links, on='link', how='left')))\n",
    "    \n",
    "    df = pd.concat([model_prep(old_data), f_df])\n",
    "    \n",
    "    cbm = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=10, silent=True)\n",
    "    \n",
    "    mod_name = 'cb_model_' + datestr + '.cbm'\n",
    "\n",
    "    cbm.fit(df[cats+nums], df['price'])\n",
    "    \n",
    "    cbm.save_model(mod_name)\n",
    "    print(f\"Model saved as '{mod_name}'.\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cols_needed = [x for x in old_data.columns.tolist() if x not in f_df.columns.tolist()]\n",
    "    \n",
    "    if len(cols_needed) != 1:\n",
    "        print(cols_needed)\n",
    "        print('error: old predictions not in ' + good_vin)    \n",
    "    '''\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587239eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.VIN.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8fe7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_data = pd.concat([df1, old_data2])\n",
    "f_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d88edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27488da",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data[f_data.VIN.duplicated()][['Make','Model','ModelYear', 'odometer','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe929439",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data.link.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links_df[~new_links_df.link.isin(old_data2.link)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557380fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a095ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = model_prep(f_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in cats+nums if x not in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cb_m.feature_names_\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = cb_m.predict(old_data2[feature_names])\n",
    "y_pred = cb_m.predict(df1[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['pred_2024_10_22'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f497d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df1['price'] holds the actual values and y_pred holds the predictions\n",
    "rmse = np.sqrt(mean_squared_error(old_data2['price'], y_pred2))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bb73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.VIN.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idk = new_df.drop_duplicates(subset=['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c860df",
   "metadata": {},
   "outputs": [],
   "source": [
    "idk.drop_duplicates(subset=['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df.link.notna()]['link'].duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([old_data, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_sql('car_data', engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.VIN.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_table(table, datestr, dbname='cars', user='postgres', password='p33Gritz!!', host='localhost', port='5432', use_sqlalchemy=True):\n",
    "   \n",
    "    backup_table_name = table + datestr\n",
    "    \n",
    "    if use_sqlalchemy:\n",
    "        # Using SQLAlchemy\n",
    "        engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "        with engine.connect() as connection:\n",
    "            with connection.begin():  # Begin a transaction\n",
    "                # Rename the table\n",
    "                connection.execute(text(f\"ALTER TABLE {table} RENAME TO {backup_table_name}\"))\n",
    "                print(f\"Table '{table}' renamed to '{backup_table_name}' using SQLAlchemy.\")\n",
    "                \n",
    "                # Retrieve the backup table as a DataFrame\n",
    "                df = pd.read_sql(text(f\"SELECT * FROM {backup_table_name}\"), connection)\n",
    "                return df  # Return the DataFrame\n",
    "\n",
    "    else:\n",
    "        # Using psycopg2\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=dbname,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        try:\n",
    "            # Rename the table\n",
    "            cur.execute(f\"ALTER TABLE {table} RENAME TO {backup_table_name}\")\n",
    "            conn.commit()  # Commit the transaction\n",
    "            print(f\"Table '{table}' renamed to '{backup_table_name}' using psycopg2.\")\n",
    "            \n",
    "            # Retrieve the backup table as a DataFrame\n",
    "            df = pd.read_sql(f\"SELECT * FROM {backup_table_name}\", conn)\n",
    "            return df  # Return the DataFrame\n",
    "            \n",
    "        except Exception as e:\n",
    "            conn.rollback()  # Rollback in case of error\n",
    "            print(f\"Error occurred: {e}\")\n",
    "        finally:\n",
    "            cur.close()\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_data = pd.read_csv('df_copy.csv', index_col=[0], dtype={'EngineCylinders': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_data.VIN.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5897978",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_data[backup_data.link.notna()]['link'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_backup = backup_data.drop_duplicates(subset=['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_f[maybe_f.VIN == 'WAU8EAF86KN024927'][['Make','Model','ModelYear', 'odometer','price', 'link', 'days_since']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7032c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "ithinkfinal.to_sql('car_data', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_f = pd.concat([good_backup, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_backup.VIN.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c45a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df1, good_backup, on='VIN', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# Filter where VIN is equal but days_since is not equal\n",
    "result_df = merged_df[merged_df['days_since_df1'] != merged_df['days_since_df2']]\n",
    "result_df[['days_since_df1', 'days_since_df2', 'price_df1','price_df2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['VIN'] == good_backup['VIN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a19e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['VIN'].isin(good_backup['VIN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7901f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ithinkfinal = maybe_f.drop_duplicates(subset=['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11125ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ithinkfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_f.VIN.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_table = backup_table('car_data_latest', date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484be075",
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = str(date.today()).replace('-','_')\n",
    "todays_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e979f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraped_links = pd.concat(scrape_regions(reg_ref))\n",
    "\n",
    "# SAve BACKUP to SQL psycopg2 etc..\n",
    "\n",
    "# df = reg_ref\n",
    "# all_links = car_data_latest\n",
    "\n",
    "def full_scrape_merge(reg_ref, car_data_latest, model=cb_latest, datetime=date.today()):\n",
    "    \n",
    "(SCRAPE)    scraped_links = pd.concat(scrape_regions(reg_ref))\n",
    "    \n",
    "1   # SAVE TO SQL \n",
    "    \n",
    "(SAVE)    'scraped_links_' + date.strftime, today\n",
    "    \n",
    "(CLEAN)    scraped_links = scraped_links.drop_duplicates(subset='link')\n",
    "    \n",
    "    repeat_mask = scraped_links['link'].isin(car_data_latest['link'])\n",
    "    need_df = clean_mask_price(scraped_links[~repeat_mask])\n",
    "    \n",
    "(MERGE)    new_links = pd.merge(need_df, reg_ref[['region_url','state','state_income','region']], on='region_url', how='left')\n",
    "    \n",
    "(SCRAPE) listing_df = scrape_listings(new_links)\n",
    "\n",
    "2   ### SAVE TO SQL \n",
    "\n",
    "(SAVE)    'new_listings_' + date.strftime, today\n",
    "    \n",
    "(CLEAN)    clean_listings = clean_listing_output(listing_df)\n",
    "    \n",
    "(MERGE)   merged_listing_links = pd.merge(clean_listings, new_links, on='link', how='left')\n",
    "\n",
    "(SCRAPE) vin_output_df = mass_decode(merged_listing_links)\n",
    "\n",
    "(SAVE) 'new_vin_output_' + date.strftime, today\n",
    "\n",
    "(CLEAN) cleaned_vin = clean_vin_output(vin_output_df)\n",
    "\n",
    "(MERGE) f_df = pd.merge(cleaned_vin, merged_listing_links, on='VIN', how='left')\n",
    "\n",
    "(SAVE OLD VERSIONS) \n",
    "\n",
    "    car_data_latest = 'car_data_backup_' + date.strftime, today\n",
    "    cb_latest = 'cb_model_backup_' + datestrftime, today\n",
    "\n",
    "(NEW DATA, NEW MODEL)\n",
    "\n",
    "new_car_data = pd.concat([f_df,car_data_latest])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984e5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_csv('no_repeats_fixed_cylinders.csv', index_col=[0], dtype={'EngineCylinders': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07184db2",
   "metadata": {},
   "source": [
    "# Step 1: Generate Links by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9253394",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ref = pd.read_csv('reg_ref_df.csv',index_col=[0])\n",
    "reg_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def scrape_regions(df):\n",
    "    dfls = []\n",
    "\n",
    "    for region_url in df.region_url.unique():\n",
    "        link = region_url + '/search/cta?auto_title_status=1&bundleDuplicates=1&query=vin#search=1~gallery~0~0'\n",
    "        tdf = df_from_link(link)\n",
    "        tdf['region_url'] = region_url\n",
    "        dfls.append(tdf)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return dfls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links = pd.concat(scrape_regions(reg_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Page 1 Every Week\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "\n",
    "def df_from_link(link):\n",
    "    response=requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Finding all listing elements with class 'cl-static-search-result'\n",
    "    listings = soup.find_all('li', class_='cl-static-search-result')\n",
    "\n",
    "    # Preparing lists to store the data\n",
    "    links = []\n",
    "    prices = []\n",
    "    locations = []\n",
    "\n",
    "    # Loop through each listing and extract the required data\n",
    "    for listing in listings:\n",
    "        # Extracting link\n",
    "        link_tag = listing.find('a', href=True)\n",
    "        link = link_tag['href'] if link_tag else 'No link'\n",
    "\n",
    "        # Extracting price\n",
    "        price_tag = listing.find('div', class_='price')\n",
    "        price = price_tag.text.strip() if price_tag else 'No price'\n",
    "\n",
    "        # Extracting location\n",
    "        location_tag = listing.find('div', class_='location')\n",
    "        location = location_tag.text.strip() if location_tag else 'No location'\n",
    "\n",
    "        # Append data to lists\n",
    "        links.append(link)\n",
    "        prices.append(price)\n",
    "        locations.append(location)\n",
    "\n",
    "    # Creating a DataFrame to store the extracted data\n",
    "    dd = pd.DataFrame({\n",
    "        'link': links,\n",
    "        'price': prices,\n",
    "        'location': locations\n",
    "    })\n",
    "\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e981f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links = pd.concat(scrape_regions(reg_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links = pd.concat(scraped_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraped_links.to_csv('scrape_proc_step1_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9a28f",
   "metadata": {},
   "source": [
    "# Step 2: Validate links: filter repeats, drop duplicates, filter price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask_price(df, mini, maxi):\n",
    "    if df['price'].dtype == 'object':\n",
    "        df['price'] = df['price'].str.replace(',', '').str.replace('$', '').astype(float)\n",
    "    return df[(df['price'] > mini) & (df['price'] < maxi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links = pd.merge(clean_mask_price(scraped_links), reg_ref, on='region_url',how='left')\n",
    "#pd.merge(new_links, reg_ref, on='region_url', how='left')\n",
    "new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = backup_table[backup_table['link'].notna()]['link'].unique()\n",
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0bffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_mask = new_links['link'].isin(link_list)\n",
    "\n",
    "need_these = new_links[~repeat_mask]\n",
    "repeats = new_links[repeat_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36521606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need_plus_region.to_csv('new_links_'+ date.today().strftime('%Y-%m-%d') + '.csv')\n",
    "#repeat_df.to_csv('repeat_links_'+ date.today().strftime('%Y-%m-%d') + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_from_dataframe(repeats, 'repeat_links', date.today())\n",
    "create_table_from_dataframe(need_these, 'new_links', date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'new_links_' + todays_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2802e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_obj = datetime.strptime(todays_date, \"%Y_%m_%d\")\n",
    "today_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20501fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listings(engine, source_table, target_table, reject_table, datestr, max_retries=3):\n",
    "    # Ensure target and reject tables exist\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Create target table if it doesn't exist\n",
    "            conn.execute(text(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {target_table} (\n",
    "                    \"VIN\" TEXT, condition TEXT, drive TEXT, fuel TEXT, odometer FLOAT,\n",
    "                    paint_color TEXT, title_status TEXT, transmission TEXT, type TEXT,\n",
    "                    posting_date TIMESTAMP, lat TEXT, long TEXT, geo_placename TEXT,\n",
    "                    geo_region TEXT, postingbody TEXT, title TEXT, link TEXT PRIMARY KEY\n",
    "                )\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(f\"Ensured that the target table '{target_table}' exists.\")\n",
    "            \n",
    "            # Create reject table with the same structure as target table\n",
    "            conn.execute(text(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {reject_table} (\n",
    "                    \"VIN\" TEXT, condition TEXT, drive TEXT, fuel TEXT, odometer FLOAT,\n",
    "                    paint_color TEXT, title_status TEXT, transmission TEXT, type TEXT,\n",
    "                    posting_date TIMESTAMP, lat TEXT, long TEXT, geo_placename TEXT,\n",
    "                    geo_region TEXT, postingbody TEXT, title TEXT, link TEXT PRIMARY KEY\n",
    "                )\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(f\"Ensured that the reject table '{reject_table}' exists.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating tables: {e}\")\n",
    "        return  # Exit if there's an issue\n",
    "\n",
    "    # Fetch remaining links to process\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            source_links_df = pd.read_sql(f\"SELECT link FROM {source_table};\", conn)\n",
    "            existing_links = pd.read_sql(f\"SELECT link FROM {target_table};\", conn)['link'].tolist()\n",
    "            rejected_links = pd.read_sql(f\"SELECT link FROM {reject_table};\", conn)['link'].tolist()\n",
    "        \n",
    "        # Filter links not yet processed or rejected\n",
    "        remaining_links = source_links_df[~source_links_df['link'].isin(existing_links + rejected_links)]['link']\n",
    "        \n",
    "        print(f\"Found {len(remaining_links)} remaining links to process.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching links from tables: {e}\")\n",
    "        return  # Exit if there's an issue\n",
    "\n",
    "    for link in remaining_links:\n",
    "        try:\n",
    "            # Run link_parser and clean output\n",
    "            parsed_df = link_parser(link)\n",
    "            if parsed_df is not None:\n",
    "                valid_df, reject_df = clean_listing_output(parsed_df)\n",
    "\n",
    "                # Insert valid listings into target table\n",
    "                with engine.connect() as conn:\n",
    "                    if not valid_df.empty:\n",
    "                        valid_df.to_sql(target_table, con=conn, if_exists='append', index=False)\n",
    "\n",
    "                    # Insert reject records into reject table with empty values for VIN and odometer\n",
    "                    if not reject_df.empty:\n",
    "                        reject_df['VIN'] = None  # Set VIN to None for rejected entries\n",
    "                        reject_df['odometer'] = None  # Set odometer to None for rejected entries\n",
    "                        reject_df.to_sql(reject_table, con=conn, if_exists='append', index=False)\n",
    "                \n",
    "                print(f\"Processed link: {link}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"Link {link} returned blocked or empty data.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error for {link}: {e}\")\n",
    "        finally:\n",
    "            time.sleep(1)  # Avoid hitting the server too quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b40136",
   "metadata": {},
   "source": [
    "# Step 3: Scrape new listings: remove: title= blocked, VIN = '', odometer filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_one(main_data, datestr, reg_ref = 'region_reference', \n",
    "            dbname='cars', user='postgres', password='p33Gritz!!', host='localhost', port='5432'):\n",
    "    \n",
    "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "    \n",
    "    # backup table : main_data + datestr\n",
    "    old_df = backup_table(main_data, datestr, dbname=dbname, user=user, password=password, host=host, port=port, use_sqlalchemy=True)\n",
    "    \n",
    "    # creates SQL tables\n",
    "    # 'repeat_links_' + datestr\n",
    "    # 'new_links_' + datestr\n",
    "    \n",
    "    scraped_links = pd.concat(scrape_regions(reg_ref))\n",
    "    sort_links(old_df, scraped_links, engine, datestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_regions(df):\n",
    "    dfls = []\n",
    "\n",
    "    for region_url in df.region_url.unique():\n",
    "        link = region_url + '/search/cta?auto_title_status=1&bundleDuplicates=1&query=vin#search=1~gallery~0~0'\n",
    "        tdf = df_from_link(link)\n",
    "        tdf['region_url'] = region_url\n",
    "        dfls.append(tdf)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return dfls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_links(backup_df, scraped_links, engine, datestr, exist='replace'):\n",
    "    \n",
    "    #link_df = pd.read_sql(link_table, engine)\n",
    "    #backup_df = pd.read_sql(backup_table, engine)\n",
    "    \n",
    "    repeat_mask = scraped_links.link.isin(backup_df.link)\n",
    "    \n",
    "    need_these = clean_mask_price(scraped_links[~repeat_mask], 4000, 110000) \n",
    "    repeats = scraped_links[repeat_mask]\n",
    "\n",
    "    df_to_table(need_these, 'new_links' + datestr, engine, exist)\n",
    "    print('new_links' + datestr + ' created')\n",
    "    df_to_table(repeats, 'repeat_links' + datestr, engine, exist)\n",
    "    print('repeat_links' + datestr + ' created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104c26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lots_stuff('car_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7412b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_links = 'new_links' + datestr\n",
    "bad_links = 'repeat_links' + datestr\n",
    "\n",
    "good_listings = 'new_listings' + datestr\n",
    "bad_listings = 'repeat_listings' + datestr\n",
    "\n",
    "good_vin_tablename = 'good_vin' + datestr\n",
    "bad_vin_tablename = 'bad_vin' + datestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a30c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_table(df, final_table_name, engine, exist = 'replace'):\n",
    "    with engine.connect() as connection:\n",
    "        df.to_sql(final_table_name, con=connection, if_exists=exist, index=False)\n",
    "        print(f\"Table '{final_table_name}' created successfully using SQLAlchemy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def table_exists(table_name, connection):\n",
    "    query = text(f\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM pg_catalog.pg_tables\n",
    "            WHERE schemaname = 'public' AND tablename = :table_name\n",
    "        );\n",
    "    \"\"\")\n",
    "    result = connection.execute(query, table_name=table_name).scalar()\n",
    "    return result\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0545245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "\n",
    "\n",
    "def table_exists(table_name, connection):\n",
    "    inspector = inspect(connection)\n",
    "    tables = inspector.get_table_names()  # Get list of table names in the current schema\n",
    "    return table_name in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datestr = '_2024_10_29'\n",
    "\n",
    "links_rejected = 'links_rejected'\n",
    "vins_rejected = 'vins_rejected'\n",
    "listings_rejected = 'listings_rejected' \n",
    "\n",
    "listings_accepted = 'listings_accepted' + datestr\n",
    "vins_accepted = 'vins_accepted' + datestr    \n",
    "links_accepted = 'links_accepted' + datestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61751d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_list = pd.read_sql(listings_accepted, engine)\n",
    "gd_link = pd.read_sql(links_accepted, engine)\n",
    "reg_ref = pd.read_sql('region_reference', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecdab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### merged_linkls = pd.merge(pd.merge(gd_list, gd_link, on='link', how='left'),reg_ref, on='region_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_linkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3975d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_gd = pd.read_sql('vins_accepted_2024_10_29', engine)\n",
    "vin_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662de466",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dropped = merged_linkls.drop_duplicates(subset=['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_no_dupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5fdddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dropped['VIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data[car_data['link'].isin(gd_link)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45634c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_no_dupe = vin_gd.drop_duplicates(subset=['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894aa579",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = pd.read_sql('car_data',engine)\n",
    "car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_car = model_prep(car_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d35cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in car_data.columns if ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pred_cb(main_data, datestr):\n",
    "    cbm = CatBoostRegressor()\n",
    "    cbm.load_model('cb_model' + datestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b27ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = cbm.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_car['days_since'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509adb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_car[cats+nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f21edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbm = CatBoostRegressor(\n",
    "    iterations=1000,          # Number of boosting iterations\n",
    "    learning_rate=0.1,       # Learning rate\n",
    "    depth=10,                 # Depth of the trees\n",
    "    silent=True,             # Silence output\n",
    "    random_seed=42           # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "cbm.fit(model_car[cats+nums], model_car['price'], cat_features=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f57bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbm.feature_names_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a942e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats+nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pred_cols = [x for x in model_car.columns if 'pred_' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_pred_model_car = model_car[[x for x in model_car.columns if 'pred_' not in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a51086",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cbm.predict(model_car[cats+nums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "datestr = '_2024_10_29'\n",
    "\n",
    "model_car['pred' + datestr] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_pred_sql = pd.read_sql('car_data_predictions', engine)\n",
    "car_pred_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_pred_sql.to_sql('car_data_predictions_2024_10_29', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45667e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(car_pred_sql, model_car[['car_id','pred_2024_10_29']], on='car_id', how='outer').to_sql('car_data_predictions', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(os.getcwd(), 'cb_models')\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cbm.save_model(os.path.join(model_dir, f'cb_model{datestr}.cbm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33141ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_no_dupe['VIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dropped[mm_dropped.VIN.isin(vn_no_dupe['VIN'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2145d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.merge(vn_no_dupe, mm_dropped,  on='VIN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930c548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df2 = model_prep(posting_date(f_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_pred, f_df2['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = cbm.predict(f_df2[cats+nums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cae5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_car.to_sql('car_data',engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b067a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df2.to_sql('car_data', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f000dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_car_id = max(model_car['car_id'].tolist())\n",
    "max_car_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330a4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ce409",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_link = pd.read_sql('links_rejected', engine)\n",
    "bd_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rej_listings[rej_listings['link'].isin(bd_link['link'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb81a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_links = gd_link[~gd_link['link'].isin(gd_list['link'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c24d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_links[~other_links['link'].isin(rej_link['link'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rej_listings = pd.read_sql('listings_rejected', engine)\n",
    "rej_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_links.to_sql('links_rejected', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rej_link = pd.read_sql('links_rejected', engine) \n",
    "rej_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df2['car_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84452f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "datestr = '_2024_10_29'\n",
    "\n",
    "f_df2['pred' + datestr] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "\n",
    "def df_to_table(df, final_table_name, engine, exist = 'replace'):\n",
    "    with engine.connect() as connection:\n",
    "        df.to_sql(final_table_name, con=connection, if_exists=exist, index=False)\n",
    "        print(f\"Table '{final_table_name}' created successfully using SQLAlchemy.\")\n",
    "        \n",
    "def scrape_regions(df):\n",
    "    dfls = []\n",
    "\n",
    "    for region_url in df.region_url.unique():\n",
    "        link = region_url + '/search/cta?auto_title_status=1&bundleDuplicates=1&query=vin#search=1~gallery~0~0'\n",
    "        tdf = df_from_link(link)\n",
    "        tdf['region_url'] = region_url\n",
    "        dfls.append(tdf)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return dfls\n",
    "\n",
    "def clean_mask_price(df, mini, maxi):\n",
    "    if df['price'].dtype == 'object':\n",
    "        df['price'] = df['price'].str.replace(',', '').str.replace('$', '').astype(float)\n",
    "    return df[(df['price'] > mini) & (df['price'] < maxi)]        \n",
    "\n",
    "\n",
    "def table_exists(table_name, connection):\n",
    "    inspector = inspect(connection)\n",
    "    tables = inspector.get_table_names()  # Get list of table names in the current schema\n",
    "    return table_name in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = current_data[current_data['link'].notna()]['link'].tolist() + current_linkrejects['link'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6eb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_links = list(set(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_linkrejects = pd.read_sql('links_rejected', engine)\n",
    "current_linkrejects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bed084",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = pd.read_sql('car_data',engine)\n",
    "current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links = pd.read_sql('links_accepted_2024_10_30', engine)\n",
    "new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_links[~new_links['link'].isin(all_true_links)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links.loc[4, 'link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d290d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://bham.craigslist.org/ctd/d/centreville-2017-gmc-sierra-1500/7797832032.html' in all_true_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_accepted = 'links_accepted' + datestr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdcdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datestr2 = '_2024_10_30'\n",
    "\n",
    "listings_accepted = 'listings_accepted' + datestr2\n",
    "listings_rejected = 'listings_rejected'\n",
    "links_rejected = 'links_rejected'\n",
    "#acclks = pd.read_sql(f'SELECT \"link\" FROM {listings_accepted};', conn)['link'].tolist()\n",
    "#acclks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_rejected_links = pd.read_sql(f'SELECT \"link\" FROM {links_rejected}', engine)['link'].tolist()\n",
    "all_links = list(set(current_data.link.tolist() + already_rejected_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cbecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link_source = link_source_df['link'].tolist()\n",
    "\n",
    "#print(len(all_links))\n",
    "#print(len(link_source))\n",
    "#print(type(link_source))\n",
    "\n",
    "# Filter links not yet processed (in case of restart) or rejected in previous scrape\n",
    "#[link for link in link_source if link not in all_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_links = pd.read_sql(f\"SELECT link FROM {listings_accepted};\", engine)['link'].tolist()\n",
    "rej_listings = pd.read_sql(f\"SELECT link FROM {listings_rejected};\", engine)['link'].tolist()\n",
    "\n",
    "all_links = list(set(all_links + listing_links + rej_listings))\n",
    "\n",
    "link_source_df = pd.read_sql(f'SELECT link FROM {links_accepted}', engine)\n",
    "\n",
    "remaining_links = link_source_df[~link_source_df['link'].isin(all_links)]['link'].tolist()\n",
    "\n",
    "print(f\"Found {len(remaining_links)} remaining links to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c21cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_goodlistings = pd.read_sql(f\"SELECT * FROM {listings_accepted};\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdls50 = current_goodlistings[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da737f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_source_df[~link_source_df['link'].isin(all_links)]['link'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_goodlistings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b6756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_goodlistings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdf = reject_more_values('links_accepted_2024_11_03', 'links_rejected', 'listings_accepted_2024_11_03', 'listings_rejected', 'vins_accepted_2024_11_03', 'region_reference', 'new_data_2024_11_03', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdf.to_sql('car_data', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c45d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir = os.path.join(os.getcwd(), 'cb_models')\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c272590",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbm = CatBoostRegressor()\n",
    "\n",
    "cbm.load_model(os.path.join(model_dir, 'cb_model_2024_10_29.cbm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17769e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cbm.predict(model_prep(dfdf[cats+nums]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_to_backup = ['car_data', 'vins_rejected', 'links_rejected', 'listings_rejected', 'schema_example','region_reference']\n",
    "\n",
    "for table in tables_to_backup:\n",
    "    # Load the table into a DataFrame\n",
    "    df = pd.read_sql_table(table, engine)\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(f\"{table}_backup.csv\", index=False)\n",
    "\n",
    "print(\"Backup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "28b892d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "syntax error at or near \"25\"\nLINE 1: CREATE TABLE IF NOT EXISTS car_data (VIN 25, odometer 701, p...\n                                                 ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[919], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     column_defs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;241m.\u001b[39mtype_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m desc \u001b[38;5;129;01min\u001b[39;00m source_cursor\u001b[38;5;241m.\u001b[39mdescription])\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Create table in destination database with the same structure\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     dest_cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE TABLE IF NOT EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_defs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     dest_conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Close all connections\u001b[39;00m\n",
      "\u001b[1;31mSyntaxError\u001b[0m: syntax error at or near \"25\"\nLINE 1: CREATE TABLE IF NOT EXISTS car_data (VIN 25, odometer 701, p...\n                                                 ^\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connection to the original database\n",
    "source_conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432, # your server hostname\n",
    "    database=\"cars\",     # your database name\n",
    "    user=\"postgres\",   # your database username\n",
    "    password=\"p33Gritz!!\" # your database password\n",
    ")\n",
    "source_cursor = source_conn.cursor()\n",
    "\n",
    "# Connection to the destination database\n",
    "dest_conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432, # your server hostname\n",
    "    database=\"cars_backup\",     # your database name\n",
    "    user=\"postgres\",   # your database username\n",
    "    password=\"p33Gritz!!\" # your database password\n",
    ")\n",
    "dest_cursor = dest_conn.cursor()\n",
    "\n",
    "# List of tables to copy\n",
    "tables_to_copy = ['car_data', 'vins_rejected', 'links_rejected', 'listings_rejected', 'schema_example','region_reference']\n",
    "\n",
    "for table in tables_to_copy:\n",
    "    # Fetch the table structure from the source database\n",
    "    source_cursor.execute(f\"SELECT * FROM {table} LIMIT 0\")\n",
    "    column_defs = ', '.join([f\"{desc.name} {desc.type_code}\" for desc in source_cursor.description])\n",
    "   \n",
    "    # Create table in destination database with the same structure\n",
    "    dest_cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table} ({column_defs})\")\n",
    "    dest_conn.commit()\n",
    "\n",
    "# Close all connections\n",
    "source_cursor.close()\n",
    "source_conn.close()\n",
    "dest_cursor.close()\n",
    "dest_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_pred, dfdf['price'],squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lots_stuff('car_data', '_2024_11_03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lots_stuff('car_data', '_2024_11_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lots_stuff('car_data_new2', '_2024_11_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lots_stuff('car_data_new', '_2024_10_31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1db735",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lots_stuff('car_data_new2', '_2024_11_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lots_stuff(main_data, datestr, reg_ref = 'region_reference',\n",
    "    dbname='cars', user='postgres', password='p33Gritz!!', host='localhost', port='5432',\n",
    "    links_rejected = 'links_rejected', vins_rejected = 'vins_rejected', listings_rejected = 'listings_rejected'):\n",
    "                  \n",
    "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "    \n",
    "    backup_tablename = main_data + datestr\n",
    "    \n",
    "    backup_query = f\"\"\"\n",
    "    CREATE TABLE \"{backup_tablename}\" AS\n",
    "    TABLE \"{main_data}\";\"\"\"\n",
    "    \n",
    "    check_query = f\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_name = '{backup_tablename}'\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    listings_accepted = 'listings_accepted' + datestr\n",
    "    vins_accepted = 'vins_accepted' + datestr    \n",
    "    links_accepted = 'links_accepted' + datestr\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            \n",
    "            result = conn.execute(text(check_query)).fetchone()\n",
    "            if result[0]:  # If the table exists\n",
    "                print(f\"Backup table '{backup_tablename}' already exists. Skipping creation.\")\n",
    "            else:\n",
    "                # Proceed to create the backup table\n",
    "                conn.execute(text(backup_query))\n",
    "                conn.commit()\n",
    "                print(f\"Backup of '{main_data}' created as '{backup_tablename}'.\")\n",
    "                \n",
    "            print(f\"Reading data from table '{main_data}'...\")\n",
    "            backup_df = pd.read_sql(f'SELECT * FROM \"{main_data}\";', conn)[['link','VIN']]\n",
    "            print(f\"Backup DataFrame shape: {backup_df.shape}\")\n",
    "            \n",
    "            reg_ref_df = pd.read_sql(reg_ref, conn)\n",
    "                    \n",
    "            check_link_query = f\"\"\"\n",
    "            SELECT EXISTS (\n",
    "                SELECT 1 \n",
    "                FROM information_schema.tables \n",
    "                WHERE table_name = '{links_accepted}'\n",
    "            );\n",
    "            \"\"\"\n",
    "            result = conn.execute(text(check_link_query)).fetchone()\n",
    "            \n",
    "            print(\"Fetching already rejected links...\")\n",
    "            already_rejected_links = pd.read_sql(f'SELECT \"link\" FROM {links_rejected}', engine)['link'].tolist()\n",
    "            all_links = list(set(backup_df.link.tolist() + already_rejected_links))\n",
    "            print('length of all rejected and in data links ' + str(len(all_links)))\n",
    "            \n",
    "            print(\"Fetching already rejected VINS...\")\n",
    "            already_rejected_vins = pd.read_sql(f'SELECT \"VIN\" FROM {vins_rejected}', engine)['VIN'].tolist()\n",
    "            all_vins = list(set(backup_df.VIN.tolist() + already_rejected_vins))\n",
    "            print('length of all rejected and in data vins ' + str(len(all_vins)))\n",
    "            \n",
    "            if result[0]:  # If the table exists\n",
    "                print(f\"Table '{links_accepted}' already exists. Skipping creation.\")\n",
    "            \n",
    "            else:\n",
    "                scraped_links = pd.concat(scrape_regions(reg_ref_df))\n",
    "      \n",
    "                repeat_mask = scraped_links.link.isin(all_links)\n",
    "                \n",
    "                # takes bulk links, removes those in accepted database and reject database\n",
    "                new_links = scraped_links[~repeat_mask].drop_duplicates(subset=['link'])\n",
    "\n",
    "                # only want price > 4000, < 110000\n",
    "                need_these_links = clean_mask_price(new_links, 4000, 110000) \n",
    "                df_to_table(need_these_links, links_accepted, engine, 'replace')\n",
    "                print(links_accepted + ' created')           \n",
    "                            \n",
    "                # add more rejects that do not match price requirements\n",
    "                reject_links = new_links[~new_links.link.isin(need_these_links.link)]\n",
    "                reject_links['date_scraped'] = datestr\n",
    "                df_to_table(reject_links, links_rejected, engine, 'append')\n",
    "                print(links_rejected + ' created')\n",
    "            \n",
    "            if not table_exists(listings_accepted, conn):\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    CREATE TABLE {listings_accepted} (\n",
    "                        \"VIN\" TEXT, condition TEXT, drive TEXT, fuel TEXT, odometer FLOAT,\n",
    "                        paint_color TEXT, title_status TEXT, transmission TEXT, type TEXT,\n",
    "                        posting_date TIMESTAMP, lat TEXT, long TEXT, geo_placename TEXT,\n",
    "                        geo_region TEXT, postingbody TEXT, title TEXT, link TEXT PRIMARY KEY\n",
    "                    )\n",
    "                \"\"\"))\n",
    "                conn.commit()\n",
    "                print(f\"Created the listing target table '{listings_accepted}'.\")\n",
    "            else:\n",
    "                print(f\"The listing target table '{listings_accepted}' already exists.\")\n",
    "\n",
    "            # Check and create good VIN table if it doesn't exist\n",
    "            if not table_exists(vins_accepted, conn):\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    CREATE TABLE \\\"{vins_accepted}\\\" AS TABLE \"schema_example\" WITH NO DATA;\n",
    "                \"\"\"))\n",
    "                print(f\"Created the good VIN table '{vins_accepted}'.\")\n",
    "                conn.commit()\n",
    "            else:\n",
    "                print(f\"The good VIN table '{vins_accepted}' already exists.\")\n",
    "    \n",
    "            # all_links contains rejected links so will not process them. adding those already in accepted in case of restart\n",
    "            #existing_links = pd.read_sql(f\"SELECT link FROM {links_accepted};\", conn)['link'].tolist()\n",
    "            listing_links = pd.read_sql(f\"SELECT link FROM {listings_accepted};\", engine)['link'].tolist()\n",
    "            rej_listings = pd.read_sql(f\"SELECT link FROM {listings_rejected};\", engine)['link'].tolist()\n",
    "\n",
    "            all_links = list(set(all_links + listing_links + rej_listings))\n",
    "\n",
    "            link_source_df = pd.read_sql(f'SELECT link FROM {links_accepted}', engine)\n",
    "\n",
    "            remaining_links = link_source_df[~link_source_df['link'].isin(all_links)]['link'].tolist()\n",
    "\n",
    "            print(f\"Found {len(remaining_links)} remaining links to process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching links from tables: {e}\")\n",
    "        return  # Exit if there's an issue\n",
    "    \n",
    "    #do_more(remaining_links, listings_accepted, listings_rejected, vins_accepted, vins_rejected, all_vins, datestr, engine)         \n",
    "    vin_batch = [] \n",
    "    for link in remaining_links:\n",
    "        try:\n",
    "            parsed_df = link_parser(link)\n",
    "            if parsed_df is not None:\n",
    "                valid_df, reject_df = clean_listing_output(parsed_df)\n",
    "                \n",
    "                with engine.connect() as conn:\n",
    "                    if not valid_df.empty:\n",
    "                        good_vin_df = pd.read_sql(f'SELECT \"VIN\" FROM {vins_accepted}', con=engine)\n",
    "                        bad_vin_df = pd.read_sql(f'SELECT \"VIN\" FROM {vins_rejected}', con=engine)\n",
    "                        all_vins += good_vin_df['VIN'].tolist() + bad_vin_df['VIN'].tolist()\n",
    "                        vin = valid_df.at[0, 'VIN']\n",
    "                        \n",
    "                        if (vin not in vin_batch) and (vin not in all_vins):\n",
    "                            valid_df.to_sql(listings_accepted, con=conn, if_exists='append', index=False)\n",
    "                            vin_batch.append(vin)\n",
    "                            print(f\"VIN added to batch: {vin}\")\n",
    "                        else:\n",
    "                            print(f\"VIN already in database: {vin}, adding to {listings_rejected}\")\n",
    "                            valid_df.to_sql(listings_rejected, con=conn, if_exists='append', index=False)\n",
    "\n",
    "                        # Process batch if full\n",
    "                        if len(vin_batch) == 50:\n",
    "                            process_vin_batch(vin_batch, engine, vins_accepted, vins_rejected, datestr)\n",
    "\n",
    "                    # Insert rejected listings\n",
    "                    if not reject_df.empty:\n",
    "                        reject_df['VIN'] = None\n",
    "                        reject_df['odometer'] = None\n",
    "                        reject_df['date_scraped'] = datestr\n",
    "                        reject_df.to_sql(listings_rejected, con=conn, if_exists='append', index=False)\n",
    "                        print(f\"Added reject records to {listings_rejected}\")\n",
    "\n",
    "                print(f\"Processed link: {link}\")\n",
    "            else:\n",
    "                print(f\"Link {link} returned blocked or empty data.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    # Process any remaining vins in the batch after all links are processed\n",
    "    if vin_batch:\n",
    "        process_vin_batch(vin_batch, engine, vins_accepted, vins_rejected, datestr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vin_batch(vin_batch, engine, vins_accepted, vins_rejected, datestr):\n",
    "    # Decode batch and clean\n",
    "    vin_df = batch_vin(';'.join(vin_batch))\n",
    "    valid_vin_df, reject_vin_df = clean_vin_output(vin_df)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        # Store valid VINs in database\n",
    "        if not valid_vin_df.empty:\n",
    "            valid_vin_df.to_sql(vins_accepted, engine, if_exists='append', index=False)\n",
    "            print(f\"Added {len(valid_vin_df)} records to {vins_accepted}\")\n",
    "\n",
    "        # Store rejected VINs in database with scrape date\n",
    "        if not reject_vin_df.empty:\n",
    "            reject_vin_df['date_scraped'] = datestr\n",
    "            reject_vin_df.to_sql(vins_rejected, engine, if_exists='append', index=False)\n",
    "            print(f\"Added {len(reject_vin_df)} records to {vins_rejected}\")\n",
    "    \n",
    "    # Clear batch after processing\n",
    "    vin_batch.clear()\n",
    "    print(\"Cleared vin_batch after processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ba130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching already rejected VINS...\")\n",
    "already_rejected_vins = pd.read_sql(f'SELECT \"VIN\" FROM {vins_rejected}', engine)['VIN'].tolist()\n",
    "all_vins = list(set(old_data.VIN.tolist() + already_rejected_vins))\n",
    "print('length of all rejected and in data vins ' + str(len(all_vins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef427850",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_links = pd.read_sql(f\"SELECT link FROM {listings_accepted};\", engine)['link'].tolist()\n",
    "rej_listings = pd.read_sql(f\"SELECT link FROM {listings_rejected};\", engine)['link'].tolist()\n",
    "\n",
    "all_links = list(set(all_links + listing_links + rej_listings))\n",
    "\n",
    "link_source_df = pd.read_sql(f'SELECT link FROM {links_accepted}', engine)\n",
    "\n",
    "remaining_links = link_source_df[~link_source_df['link'].isin(all_links)]['link'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb6f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_more(remaining_links, listings_accepted, listings_rejected, vins_accepted, vins_rejected, all_vins, datestr, engine):\n",
    "\n",
    "    vin_batch = [] \n",
    "\n",
    "    for link in remaining_links:\n",
    "        try:\n",
    "            parsed_df = link_parser(link)\n",
    "            if parsed_df is not None:\n",
    "                valid_df, reject_df = clean_listing_output(parsed_df)\n",
    "\n",
    "                # Insert valid listings into target table\n",
    "                with engine.connect() as conn:\n",
    "                    if not valid_df.empty:\n",
    "                        good_vin_df = pd.read_sql(f'SELECT \"VIN\" FROM {vins_accepted}', con=engine)\n",
    "                        bad_vin_df = pd.read_sql(f'SELECT \"VIN\" FROM {vins_rejected}', con=engine)\n",
    "\n",
    "                        # Concatenate VIN values from all tables into a single list\n",
    "                        all_vins = all_vins + good_vin_df['VIN'].tolist() + bad_vin_df['VIN'].tolist()\n",
    "\n",
    "                        vin = valid_df.at[0, 'VIN']\n",
    "\n",
    "                        if (vin not in vin_batch) and (vin not in all_vins):\n",
    "                            valid_df.to_sql(listings_accepted, con=conn, if_exists='append', index=False)\n",
    "                            vin_batch.append(vin)\n",
    "                            print(f\"VIN added to batch: {vin}\")\n",
    "\n",
    "                        else:\n",
    "                            print(f\"VIN already in database: {vin}, added to {listings_rejected}\")\n",
    "                            valid_df.to_sql(listings_rejected, con=conn, if_exists='append', index=False)\n",
    "\n",
    "                        # If batch is full, process it\n",
    "                        if len(vin_batch) == 50:\n",
    "                            print(\"Processing full batch:\")\n",
    "                            print(len(vin_batch))\n",
    "\n",
    "                            # Decode batch and clean\n",
    "                            vin_df = batch_vin(';'.join(vin_batch))\n",
    "                            valid_vin_df, reject_vin_df = clean_vin_output(vin_df)\n",
    "\n",
    "                            # Store valid VINs in database\n",
    "                            valid_vin_df.to_sql(vins_accepted, engine, if_exists='append', index=False)\n",
    "                            print(f\"Added {len(valid_vin_df)} records to {vins_accepted}\")\n",
    "\n",
    "                            # Store rejected VINs in database with scrape date\n",
    "                            reject_vin_df['date_scraped'] = datestr\n",
    "                            reject_vin_df.to_sql(vins_rejected, engine, if_exists='append', index=False)\n",
    "                            print(f\"Added {len(reject_vin_df)} records to {vins_rejected}\")\n",
    "\n",
    "                            # Clear batch after processing\n",
    "                            vin_batch.clear()\n",
    "                            print(\"Cleared vin_batch after processing.\")\n",
    "                            print(len(vin_batch))\n",
    "\n",
    "                    # Insert reject records\n",
    "                    if not reject_df.empty:     \n",
    "                        reject_df['VIN'] = None\n",
    "                        reject_df['odometer'] = None\n",
    "                        reject_df['date_scraped'] = datestr\n",
    "\n",
    "                        reject_df.to_sql(listings_rejected, con=conn, if_exists='append', index=False)\n",
    "                        print(f\"Added reject records to {listings_rejected}\")\n",
    "\n",
    "                print(f\"Processed link: {link}\")\n",
    "            else:\n",
    "                print(f\"Link {link} returned blocked or empty data.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {link}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            time.sleep(1)  # Avoid hitting the server too quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9aeb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aba1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_new_id(old_data, new_data):\n",
    "    max_car_id = max(old_data['car_id'].tolist())\n",
    "    new_data['car_id'] = range(max_car_id + 1, max_car_id + 1 + len(new_data))\n",
    "    \n",
    "    return pd.concat([old_data, new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_accepted = 'listings_accepted_2024_10_30'\n",
    "listings_rejected = 'listings_rejected'\n",
    "vins_rejected = 'vins_rejected'\n",
    "\n",
    "\n",
    "def bad_vin_bad_listing(listings_accepted, listings_rejected, vins_rejected, engine):\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        try:\n",
    "            # Step 1: Fetch all unique rejected VINs from 'vins_rejected'\n",
    "            bad_vins = pd.read_sql(f'SELECT \"VIN\" FROM {vins_rejected}', conn)['VIN'].unique()\n",
    "\n",
    "            # Step 2: Identify listings in 'listings_accepted' that have rejected VINs\n",
    "            # Convert list of bad_vins into a format suitable for SQL WHERE IN clause\n",
    "            bad_vins_str = \"', '\".join(bad_vins)\n",
    "\n",
    "            # Fetch rows from listings_accepted that have these rejected VINs\n",
    "            rejected_listings_query = f\"\"\"\n",
    "                SELECT * \n",
    "                FROM {listings_accepted}\n",
    "                WHERE \"VIN\" IN ('{bad_vins_str}')\n",
    "            \"\"\"\n",
    "            rejected_listings = pd.read_sql(rejected_listings_query, conn)\n",
    "\n",
    "            # Step 3: Insert the rows with rejected VINs into 'listings_rejected'\n",
    "            if not rejected_listings.empty:\n",
    "                rejected_listings.to_sql(listings_rejected, conn, if_exists='append', index=False)\n",
    "                print(f\"Moved {len(rejected_listings)} records to '{listings_rejected}'.\")\n",
    "\n",
    "            # Step 4: Delete rows with rejected VINs from 'listings_accepted'\n",
    "            delete_query = f\"\"\"\n",
    "                DELETE FROM {listings_accepted}\n",
    "                WHERE \"VIN\" IN ('{bad_vins_str}')\n",
    "            \"\"\"\n",
    "            conn.execute(text(delete_query))\n",
    "            print(f\"Removed {len(rejected_listings)} records from '{listings_accepted}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad420341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('vins_rejected', engine)['VIN'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_bad_links(vin_rejects):\n",
    "    \n",
    "    if vin_rejects['vin'].isin(listing_rejects['vin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e01c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_vin(vin_input):\n",
    "    url = 'https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVINValuesBatch/'\n",
    "    post_fields = {'format': 'json', 'data': vin_input}\n",
    "    r = requests.post(url, data=post_fields)\n",
    "    vin_return = json.loads(r.text)\n",
    "    return pd.DataFrame(vin_return['Results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_goodlistings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = np.arange(0, 482, 50)\n",
    "ends = np.append(np.arange(50, 482, 50), 482)\n",
    "cur_pairs = dict(zip(starts, ends))\n",
    "cur_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "vins_accepted = 'vins_accepted_2024_10_30'\n",
    "vins_rejected = 'vins_rejected'\n",
    "datestr = '_2024_10_30'\n",
    "\n",
    "for k,v in cur_pairs.items():\n",
    "    \n",
    "    vin_df = batch_vin(';'.join(current_goodlistings[k:v]['VIN'].tolist()))\n",
    "\n",
    "    valid_vin_df, reject_vin_df = clean_vin_output(vin_df)\n",
    "\n",
    "    valid_vin_df.to_sql(vins_accepted, engine, if_exists='append', index=False)\n",
    "    \n",
    "    reject_vin_df['date_scraped'] = datestr\n",
    "    reject_vin_df.to_sql(vins_rejected, engine, if_exists='append', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c913cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "badvins = pd.read_sql(vins_rejected, engine)\n",
    "#badvins\n",
    "badvins.drop_duplicates(subset=['VIN']).to_sql(vins_rejected, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "badvins.VIN.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b900ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gudvins = pd.read_sql(vins_accepted, engine)\n",
    "gudvins.drop_duplicates(subset=['VIN']).to_sql(vins_accepted, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b941ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "';'.join(current_goodlistings[50:100]['VIN'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_batch = []\n",
    "\n",
    "\n",
    "for \n",
    "vin_batch.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "';'.join(gdls50['VIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_df = batch_vin(';'.join(gdls50['VIN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vin_output(df):\n",
    "    # Replace empty strings with 'nan' (if you want actual NaNs, use np.nan instead of 'nan')\n",
    "    df = df.replace('', 'nan')\n",
    "\n",
    "    # Separate rows with invalid ErrorCodes, or nulls in key columns, into df_bad\n",
    "    df_bad = df[~df['ErrorCode'].isin(['0', '1', '6']) | df[['Make', 'Model', 'ModelYear', 'VIN']].isnull().any(axis=1)]\n",
    "    df_good = df[df['ErrorCode'].isin(['0', '1', '6'])].dropna(subset=['Make', 'Model', 'ModelYear', 'VIN'])\n",
    "\n",
    "    # Define the value filter for acceptable vehicle types and body classes\n",
    "    value_filter = (\n",
    "        df_good['VehicleType'].isin(['TRUCK', 'MULTIPURPOSE PASSENGER VEHICLE (MPV)', 'PASSENGER CAR']) &\n",
    "        df_good['BodyClass'].isin(['Pickup', 'Sport Utility Vehicle (SUV)/Multi-Purpose Vehicle (MPV)',\n",
    "                                   'Crossover Utility Vehicle (CUV)', 'Sedan/Saloon',\n",
    "                                   'Hatchback/Liftback/Notchback', 'Coupe', 'Convertible/Cabriolet',\n",
    "                                   'Minivan', 'Wagon', 'Cargo Van', 'Van'])\n",
    "    )\n",
    "\n",
    "    # Apply the value filter, adding rows that don't meet it to df_bad\n",
    "    df2 = df_good[value_filter]\n",
    "    df_bad = pd.concat([df_bad, df_good[~value_filter]], ignore_index=True)\n",
    "\n",
    "    return df2, df_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_vin_df, reject_vin_df = clean_vin_output(vin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f535bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_vin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a46eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape_listings('new_links_' + todays_date, 'new_listings_' + todays_date, 'reject_listings_' + todays_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_everything('modern_car_data', datestr='2024_10_28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ead27",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = 'car_data', datestr = '_2024_10_28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dcefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_retrain_update(old_data, good_vin, new_listings, new_links, datestr, cats=cats, nums=nums):\n",
    "    \n",
    "    vin_plus = pd.merge(good_vin, new_listings)\n",
    "    f_df = model_prep(posting_date(pd.merge(vin_plus, new_links, on='link', how='left')))\n",
    "    \n",
    "    df = pd.concat([model_prep(old_data), f_df])\n",
    "    \n",
    "    cbm = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=10, silent=True)\n",
    "    \n",
    "    mod_name = 'cb_model_' + datestr + '.cbm'\n",
    "\n",
    "    cbm.fit(df[cats+nums], df['price'])\n",
    "    \n",
    "    cbm.save_model(mod_name)\n",
    "    print(f\"Model saved as '{mod_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1402536",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_sql_query('SELECT \"VIN\" from good_vin_2024_10_28 LIMIT 50', engine)\n",
    "test_vins = ';'.join(f['VIN'].tolist())\n",
    "test_vins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = batch_vin('3N6CM0KN1LK692922;1C4HJXEG1LW182629;1GCWGAFG2K1209883;1GCHSCEN8L1148415;3KPF24AD9PE588320;4T1G11AK6NU653655;2C4RDGEG4LR160211;3C6RR6KG4MG547582;3N6CM0KN0MK700980;1C6RRFFG1MN748244;3TMAZ5CN7KM104659;1N4BL4CV8PN350381;1GNSKBKC8GR321688;5XYP24HC8NG303107;2C4RC1DG5MR538748;1GCRYAEF6MZ136458;1C6SRFBTXNN164494;1GCHSBEN5L1109498;JTMZFREV6JJ747614;1FM5K7D89JGA51624;3GTU2LEC4HG200354;KL4CJBSB7DB096296;4T1K61AK4PU145540;2C4RC1BG3NR117602;5TFCZ5AN1HX057921;JTEBU5JR6F5228664;1N4BL4DV4PN305386;5XYRG4LC7NG097392;KL79MPS29NB125008;1GCGSBEN7N1154585;5TDYK3EH2CS084532;3N1AB8CV5NY228497;5YFEPMAE4NP365748;3N1CP5BV3ML479573;JTDKN3DU2B1409686;5GAKVCED1BJ186654;5TFHW5F14BX196517;3GTU2MEC9HG346609;JTNKHMBX0M1094542;2GCVKPEC7K1100454;2C3CDZAG3HH665410;5YFB4MDE5PP033215;4T1B11HK6KU763682;7MUCAAAG5NV010268;3KPF24AD3PE564627;2T2AZMDA6LC245752;5TFHY5F11MX007285;1N4AA6FV9LC373196;5N1DR3BA3NC216067;3TYCZ5AN2MT032658')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0855d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tdf['Results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "\n",
    "def df_to_table(df, final_table_name, engine, exist = 'replace'):\n",
    "    with engine.connect() as connection:\n",
    "        df.to_sql(final_table_name, con=connection, if_exists=exist, index=False)\n",
    "        print(f\"Table '{final_table_name}' created successfully using SQLAlchemy.\")\n",
    "        \n",
    "def scrape_regions(df):\n",
    "    dfls = []\n",
    "\n",
    "    for region_url in df.region_url.unique():\n",
    "        link = region_url + '/search/cta?auto_title_status=1&bundleDuplicates=1&query=vin#search=1~gallery~0~0'\n",
    "        tdf = df_from_link(link)\n",
    "        tdf['region_url'] = region_url\n",
    "        dfls.append(tdf)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return dfls\n",
    "\n",
    "def clean_mask_price(df, mini, maxi):\n",
    "    if df['price'].dtype == 'object':\n",
    "        df['price'] = df['price'].str.replace(',', '').str.replace('$', '').astype(float)\n",
    "    return df[(df['price'] > mini) & (df['price'] < maxi)]        \n",
    "\n",
    "\n",
    "def table_exists(table_name, connection):\n",
    "    inspector = inspect(connection)\n",
    "    tables = inspector.get_table_names()  # Get list of table names in the current schema\n",
    "    return table_name in tables\n",
    "\n",
    "def batch_vin(vin_input):\n",
    "    url = 'https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVINValuesBatch/'\n",
    "    post_fields = {'format': 'json', 'data': vin_input}\n",
    "    r = requests.post(url, data=post_fields)\n",
    "    vin_return = json.loads(r.text)\n",
    "    return pd.DataFrame(vin_return['Results'])\n",
    "\n",
    "\n",
    "def clean_vin_output(df):\n",
    "    \n",
    "    df = df[df['ErrorCode'].isin(['0', '1', '6'])]\n",
    "    #df = posting_date(df)  # Assuming posting_date is defined elsewhere\n",
    "    df = df.replace('', 'nan')\n",
    "\n",
    "    \n",
    "    #df[cats] = df[cats].astype(str)\n",
    "    #df[nums] = df[nums].astype('float64')\n",
    "    \n",
    "    df = df.dropna(subset=['Make', 'Model', 'ModelYear', 'VIN'])\n",
    "\n",
    "    \n",
    "    value_filter = (df.VehicleType.isin(['TRUCK', 'MULTIPURPOSE PASSENGER VEHICLE (MPV)', 'PASSENGER CAR'])) & (df['BodyClass'].isin(['Pickup', 'Sport Utility Vehicle (SUV)/Multi-Purpose Vehicle (MPV)',\n",
    "       'Crossover Utility Vehicle (CUV)', 'Sedan/Saloon',\n",
    "       'Hatchback/Liftback/Notchback', 'Coupe', 'Convertible/Cabriolet',\n",
    "       'Minivan', 'Wagon', 'Cargo Van', 'Van']))\n",
    "       \n",
    "    df2 = df[value_filter]\n",
    "    df_bad = df[~value_filter]\n",
    "\n",
    "    return df2, df_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fe3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "    return {}\n",
    "\n",
    "def vin_decode(vin):\n",
    "    json_data = get_json(f'https://vpic.nhtsa.dot.gov/api/vehicles/decodevinvalues/{vin}?format=json')\n",
    "    \n",
    "    if json_data['Results']:\n",
    "        df = pd.DataFrame(json_data['Results'])\n",
    "    else: \n",
    "        df = pd.DataFrame(columns=cats + nums)\n",
    "    return df\n",
    "\n",
    "def clean_vin_output(df):\n",
    "    df = df[df['ErrorCode'].isin(['0', '1'])]\n",
    "    #df = posting_date(df)  # Assuming posting_date is defined elsewhere\n",
    "    df = df.replace('', 'nan')\n",
    "\n",
    "    #df[cats] = df[cats].astype(str)\n",
    "    #df[nums] = df[nums].astype('float64')\n",
    "    \n",
    "    df = df.dropna(subset=['Make', 'Model', 'ModelYear', 'VIN'])\n",
    "    \n",
    "    value_filter = (df.VehicleType.isin(['TRUCK', 'MULTIPURPOSE PASSENGER VEHICLE (MPV)', 'PASSENGER CAR'])) & \n",
    "    (df['BodyClass'].isin(['Pickup', 'Sport Utility Vehicle (SUV)/Multi-Purpose Vehicle (MPV)',\n",
    "       'Crossover Utility Vehicle (CUV)', 'Sedan/Saloon',\n",
    "       'Hatchback/Liftback/Notchback', 'Coupe', 'Convertible/Cabriolet',\n",
    "       'Minivan', 'Wagon', 'Cargo Van', 'Van']))\n",
    "       \n",
    "    df2 = df[value_filter]\n",
    "    df_bad = df[~value_filter]\n",
    "    \n",
    "    return df2, df_bad\n",
    "\n",
    "def mass_decode(source_table, target_table, reject_table, dbname='cars', user='postgres', password='p33Gritz!!', host='localhost', port='5432'):\n",
    "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Create target and reject tables using schema from schema_example only if they do not exist\n",
    "            conn.execute(text(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS \\\"{target_table}\\\" AS TABLE \"schema_example\" WITH NO DATA;\n",
    "            \"\"\"))\n",
    "            conn.execute(text(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS \\\"{reject_table}\\\" AS TABLE \"schema_example\" WITH NO DATA;\n",
    "            \"\"\"))\n",
    "            print(f\"Ensured that the target table '{target_table}' and reject table '{reject_table}' exist.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating tables: {e}\")\n",
    "        return  # Exit if there's an issue\n",
    "\n",
    "    # Fetch VINs from source_table\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            source_vins = pd.read_sql(f'SELECT \"VIN\" FROM \"{source_table}\";', conn)\n",
    "            existing_vins = pd.read_sql(f'SELECT \"VIN\" FROM \"{target_table}\";', conn)['VIN'].tolist() if conn.dialect.has_table(conn, target_table) else []\n",
    "            rejected_vins = pd.read_sql(f'SELECT \"VIN\" FROM \"{reject_table}\";', conn)['VIN'].tolist() if conn.dialect.has_table(conn, reject_table) else []\n",
    "\n",
    "        # Filter VINs that are already processed or rejected\n",
    "        remaining_vins = source_vins[~source_vins['VIN'].isin(existing_vins + rejected_vins)]\n",
    "        \n",
    "        print(f\"Found {len(remaining_vins)} remaining VINs to process.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching VINs from tables: {e}\")\n",
    "        return  # Exit if there's an issue\n",
    "\n",
    "    # Process each remaining VIN\n",
    "    for _, row in remaining_vins.iterrows():\n",
    "        vin = row['VIN']\n",
    "        if pd.isna(vin):  # Skip if VIN is not available\n",
    "            print(f\"Skipping VIN {row['VIN']} due to missing VIN.\")\n",
    "            continue\n",
    "        \n",
    "        df = vin_decode(vin)\n",
    "        \n",
    "        # Check if we need to overwrite VIN with a new value\n",
    "        vin2 = df.loc[0, 'VIN'] if not df.empty else None\n",
    "        if vin2 and vin2 != vin:\n",
    "            remaining_vins.loc[remaining_vins['VIN'] == vin, 'VIN'] = vin2  # Overwrite the original VIN\n",
    "\n",
    "        # Clean the output\n",
    "        valid_df, reject_df = clean_vin_output(df)\n",
    "        # Insert valid entries into target_table\n",
    "        if not valid_df.empty:\n",
    "            valid_df.to_sql(target_table, con=engine, if_exists='append', index=False, method='multi')  # Insert valid entries\n",
    "            print(f\"Inserted {len(valid_df)} valid records for VIN '{vin}' into target table '{target_table}'.\")\n",
    "\n",
    "        # Insert rejected entries into reject_table\n",
    "        if not reject_df.empty:\n",
    "            reject_df['VIN'] = vin  # Set the original VIN for rejected entries\n",
    "            reject_df.to_sql(reject_table, con=engine, if_exists='append', index=False, method='multi')  # Insert rejected entries\n",
    "            print(f\"Inserted {len(reject_df)} rejected records for VIN '{vin}' into reject table '{reject_table}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_sql(main_data, 'region_reference', date.today(), etc...):\n",
    "    \n",
    "    #1 backup data-> save links in table to list\n",
    "    \n",
    "    create 'car_data_' + date.today()\n",
    "    \n",
    "    new_links: scrape region_df for links, table with all DATA merged w/ regions\n",
    "    reject_links: already in db / bad price\n",
    "        \n",
    "    '''\n",
    "    scraped_links = pd.concat(scrape_regions(reg_ref))\n",
    "    new_links = pd.merge(clean_mask_price(scraped_links), reg_ref, on='region_url',how='left')\n",
    "    link_list = backup_table[backup_table['link'].notna()]['link'].unique()\n",
    "\n",
    "    repeat_mask = new_links['link'].isin(link_list)\n",
    "\n",
    "    need_these = new_links[~repeat_mask]\n",
    "    repeats = new_links[repeat_mask]\n",
    "\n",
    "    create_table_from_dataframe(repeats, 'repeat_links', date.today())\n",
    "    create_table_from_dataframe(need_these, 'new_links', date.today())\n",
    "\n",
    "    '''\n",
    "        \n",
    "    scrape_listings( new_links_ + date.today(),  \n",
    "                    new_listings_ + date.today() (important string, tablename),\n",
    "                    reject_listings)\n",
    "    \n",
    "    for every listing that is processed:\n",
    "        divide into rejects\n",
    "        \n",
    "        mass_decode(new_listings_ + todays_date (important string, tablename) (but not so mass...)\n",
    "        creates: good_vin, bad_vin\n",
    "                    \n",
    "    merge good_vin on new_listings on 'VIN', left\n",
    "    merge that with new_links on 'link', left\n",
    "                    \n",
    "    creates df\n",
    "    \n",
    "    df[cats].astype(str)\n",
    "    df[nums].astype('float64')\n",
    "    \n",
    "    #copy 'car_data_' + date.today() table -> append df (might be tricky...)\n",
    "    \n",
    "    UNION df with 'car_data_backup'\n",
    "                    \n",
    "    name 'car_data'\n",
    "                    \n",
    "    save backup model\n",
    "    train new model\n",
    "    replace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecb44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(pd.read_sql('new_listings_' + todays_date), pd.read_sql('new_links_' + todays_date), on='link', how='left')\n",
    "\n",
    "#scrape_listings('new_links_' + todays_date, 'new_listings_' + todays_date, 'reject_listings_' + todays_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "\n",
    "def df_to_table(df, final_table_name, engine, exist = 'replace'):\n",
    "    with engine.connect() as connection:\n",
    "        df.to_sql(final_table_name, con=connection, if_exists=exist, index=False)\n",
    "        print(f\"Table '{final_table_name}' created successfully using SQLAlchemy.\")\n",
    "        \n",
    "def scrape_regions(df):\n",
    "    dfls = []\n",
    "\n",
    "    for region_url in df.region_url.unique():\n",
    "        link = region_url + '/search/cta?auto_title_status=1&bundleDuplicates=1&query=vin#search=1~gallery~0~0'\n",
    "        tdf = df_from_link(link)\n",
    "        tdf['region_url'] = region_url\n",
    "        dfls.append(tdf)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return dfls\n",
    "\n",
    "def clean_mask_price(df, mini, maxi):\n",
    "    if df['price'].dtype == 'object':\n",
    "        df['price'] = df['price'].str.replace(',', '').str.replace('$', '').astype(float)\n",
    "    return df[(df['price'] > mini) & (df['price'] < maxi)]        \n",
    "\n",
    "\n",
    "def table_exists(table_name, connection):\n",
    "    inspector = inspect(connection)\n",
    "    tables = inspector.get_table_names()  # Get list of table names in the current schema\n",
    "    return table_name in tables\n",
    "\n",
    "def batch_vin(vin_input):\n",
    "    url = 'https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVINValuesBatch/'\n",
    "    post_fields = {'format': 'json', 'data': vin_input}\n",
    "    r = requests.post(url, data=post_fields)\n",
    "    vin_return = json.loads(r.text)\n",
    "    return pd.DataFrame(vin_return['Results'])\n",
    "\n",
    "\n",
    "def clean_vin_output(df):\n",
    "    \n",
    "    df = df[df['ErrorCode'].isin(['0', '1', '6'])]\n",
    "    #df = posting_date(df)  # Assuming posting_date is defined elsewhere\n",
    "    df = df.replace('', 'nan')\n",
    "\n",
    "    \n",
    "    #df[cats] = df[cats].astype(str)\n",
    "    #df[nums] = df[nums].astype('float64')\n",
    "    \n",
    "    df = df.dropna(subset=['Make', 'Model', 'ModelYear', 'VIN'])\n",
    "\n",
    "    \n",
    "    value_filter = (df.VehicleType.isin(['TRUCK', 'MULTIPURPOSE PASSENGER VEHICLE (MPV)', 'PASSENGER CAR'])) & (df['BodyClass'].isin(['Pickup', 'Sport Utility Vehicle (SUV)/Multi-Purpose Vehicle (MPV)',\n",
    "       'Crossover Utility Vehicle (CUV)', 'Sedan/Saloon',\n",
    "       'Hatchback/Liftback/Notchback', 'Coupe', 'Convertible/Cabriolet',\n",
    "       'Minivan', 'Wagon', 'Cargo Van', 'Van']))\n",
    "       \n",
    "    df2 = df[value_filter]\n",
    "    df_bad = df[~value_filter]\n",
    "\n",
    "    return df2, df_bad\n",
    "\n",
    "def link_parser(link):\n",
    "\n",
    "    response=requests.get(link)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Dictionary to hold the data\n",
    "    data = {}\n",
    "\n",
    "    # Define expected fields (labels) and initialize all as None\n",
    "    fields = ['VIN:', 'condition:', 'drive:', 'fuel:', 'odometer:', 'paint color:', 'title status:', 'transmission:', 'type:']\n",
    "    data = {field: None for field in fields}\n",
    "\n",
    "    title_tag = soup.find('title')\n",
    "        \n",
    "    if title_tag:\n",
    "        title = title_tag.text.strip()\n",
    "        if title == 'blocked':\n",
    "            return None\n",
    "    else:\n",
    "        title = None  # Just in case no title tag is found (shouldn't happen)\n",
    "        \n",
    "    time_tag = soup.find('time', class_='date timeago')\n",
    "    if time_tag and 'datetime' in time_tag.attrs:\n",
    "        posting_date_str = time_tag['datetime']\n",
    "        try:\n",
    "            # Use strptime for ISO format: \"2024-10-18T17:06:07-0500\"\n",
    "            posting_date = datetime.strptime(posting_date_str, '%Y-%m-%dT%H:%M:%S%z')\n",
    "        except ValueError:\n",
    "            posting_date = None\n",
    "        data['posting_date'] = posting_date\n",
    "    else:\n",
    "        data['posting_date'] = None\n",
    "        \n",
    "    geo_position_tag = soup.find('meta', attrs={'name': 'geo.position'})\n",
    "    if geo_position_tag:\n",
    "        geo_position = geo_position_tag.get('content', None)\n",
    "        if geo_position:\n",
    "            lat, long = geo_position.split(';')\n",
    "            data['lat'] = lat\n",
    "            data['long'] = long\n",
    "    else:\n",
    "        data['lat'] = None\n",
    "        data['long'] = None\n",
    "\n",
    "    geo_placename_tag = soup.find('meta', attrs={'name': 'geo.placename'})\n",
    "    if geo_placename_tag:\n",
    "        data['geo_placename'] = geo_placename_tag.get('content', None)\n",
    "    else:\n",
    "        data['geo_placename'] = None\n",
    "\n",
    "    geo_region_tag = soup.find('meta', attrs={'name': 'geo.region'})\n",
    "    if geo_region_tag:\n",
    "        data['geo_region'] = geo_region_tag.get('content', None)\n",
    "    else:\n",
    "        data['geo_region'] = None\n",
    "        \n",
    "    posting_body_section = soup.find('section', id='postingbody')\n",
    "    if posting_body_section:\n",
    "        h2_tag = posting_body_section.find('h2')\n",
    "        if h2_tag:\n",
    "            # Clean text by removing HTML tags and special characters\n",
    "            posting_body = h2_tag.get_text(strip=True)\n",
    "            posting_body = re.sub(r'[^a-zA-Z0-9\\s]', '', posting_body)  # Remove special characters\n",
    "            data['postingbody'] = posting_body\n",
    "        else:\n",
    "            data['postingbody'] = None\n",
    "    else:\n",
    "        data['postingbody'] = None\n",
    "        \n",
    "        \n",
    "    # Finding only the relevant divs with class \"attrgroup\"\n",
    "    attr_groups = soup.find_all('div', class_='attrgroup')\n",
    "\n",
    "    # Loop through each attrgroup and extract 'labl' and 'valu'\n",
    "    for group in attr_groups:\n",
    "        attrs = group.find_all('div', class_='attr')  # Find individual attributes in the group\n",
    "        for attr in attrs:\n",
    "            labl_tag = attr.find('span', class_='labl')\n",
    "            valu_tag = attr.find('span', class_='valu')\n",
    "\n",
    "            # Check if both labl and valu are present\n",
    "            if labl_tag and valu_tag:\n",
    "                labl = labl_tag.text.strip()  # Keep the original labl with the colon\n",
    "                valu = valu_tag.text.strip()\n",
    "\n",
    "                # Ensure we're only storing known fields\n",
    "                if labl in data:\n",
    "                    data[labl] = valu\n",
    "                    \n",
    "    data['title'] = title\n",
    "    data['link'] = link\n",
    "    \n",
    "    ff = pd.DataFrame([data])\n",
    "    return ff\n",
    "\n",
    "def clean_listing_output(df2):\n",
    "    # Column renaming dictionary\n",
    "    column_rename_dc = {\n",
    "        'VIN:': 'VIN', 'condition:': 'condition', 'drive:': 'drive', 'fuel:': 'fuel',\n",
    "        'paint color:': 'paint_color', 'type:': 'type', 'transmission:': 'transmission',\n",
    "        'title status:': 'title_status', 'odometer:': 'odometer'\n",
    "    }\n",
    "    \n",
    "    # Rename and clean DataFrame\n",
    "    df = df2.rename(columns=column_rename_dc).reset_index(drop=True)\n",
    "    df = df.replace('', 'nan')\n",
    "\n",
    "    if 'odometer' in df and df['odometer'].dtype == 'object':\n",
    "        df['odometer'] = df['odometer'].str.replace(',', '').astype(float)\n",
    "\n",
    "    # Truncate VIN to 16 characters if longer, else mark for rejection if less than 16\n",
    "    df['VIN'] = df['VIN'].str[:16]\n",
    "    \n",
    "    # Filter listings with valid VIN and odometer\n",
    "    valid_df = df[(df['VIN'].notnull()) & (df['VIN'].str.len() == 16) & (df['odometer'].between(25000, 310000))]\n",
    "    reject_df = df[~df.index.isin(valid_df.index)]  # Entries that don't meet criteria\n",
    "\n",
    "    return valid_df, reject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_listing_output(df2):\n",
    "    # Column renaming dictionary\n",
    "    column_rename_dc = {\n",
    "        'VIN:': 'VIN', 'condition:': 'condition', 'drive:': 'drive', 'fuel:': 'fuel',\n",
    "        'paint color:': 'paint_color', 'type:': 'type', 'transmission:': 'transmission',\n",
    "        'title status:': 'title_status', 'odometer:': 'odometer'\n",
    "    }\n",
    "    \n",
    "    # Rename and clean DataFrame\n",
    "    df = df2.rename(columns=column_rename_dc).reset_index(drop=True)\n",
    "    df = df.replace('', 'nan')\n",
    "\n",
    "    if 'odometer' in df and df['odometer'].dtype == 'object':\n",
    "        df['odometer'] = df['odometer'].str.replace(',', '').astype(float)\n",
    "\n",
    "    # Truncate VIN to 16 characters if longer, else mark for rejection if less than 16\n",
    "    df['VIN'] = df['VIN'].str[:16]\n",
    "    \n",
    "    # Filter listings with valid VIN and odometer\n",
    "    valid_df = df[(df['VIN'].notnull()) & (df['VIN'].str.len() == 16) & (df['odometer'].between(25000, 310000))]\n",
    "    reject_df = df[~df.index.isin(valid_df.index)]  # Entries that don't meet criteria\n",
    "\n",
    "    return valid_df, reject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf121b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_listing_output1(df2):\n",
    "    \n",
    "    column_rename_dc = {'VIN:': 'VIN',\n",
    "                   'condition:' : 'condition',\n",
    "                   'drive:': 'drive',\n",
    "                   'fuel:' : 'fuel',\n",
    "                   'paint color:' : 'paint_color',\n",
    "                   'type:' : 'type',\n",
    "                   'transmission:' : 'transmission',\n",
    "                   'title status:': 'title_status',\n",
    "                   'odometer:' : 'odometer'}\n",
    "\n",
    "    df = df2.rename(columns=column_rename_dc).reset_index(drop=True)\n",
    "    df = df[(df['odometer'] != '') & (df['VIN'] != '')]\n",
    "    df = df.replace('','nan')\n",
    "    \n",
    "    df['VIN'] = df['VIN'].astype(str)\n",
    "    df = df[(df['VIN'].astype(str).str.len() > 16)]\n",
    "    \n",
    "    if df['odometer'].dtype == 'object':\n",
    "        df['odometer'] = df['odometer'].str.replace(',', '').astype(float)\n",
    "        \n",
    "    listing_df = df[(df['VIN'] != '') & (df['odometer'] > 25000) & (df['odometer'] < 310000) & (df['VIN'].astype(str).str.len() > 16)].drop_duplicates(subset='VIN')\n",
    "    return listing_df, df2[~df2.index.isin(listing_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = clean_listing_output(full_listing_df)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df[listing_df['VIN'] == 'WAUDT48H14K020081']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_listing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64948d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df = pd.merge(xx, need_plus_region, on='link', how='left')\n",
    "listing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eead74",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df[listing_df.VIN.str.contains('WAUDT48H14K020081')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627753e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing_df.to_csv('listings_2024_10_21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc40f9",
   "metadata": {},
   "source": [
    "# Step 4: Listing VIN Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c745131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vin_output(df):\n",
    "    df = df[df['ErrorCode'].isin(['0','1'])]\n",
    "    df = posting_date(df)\n",
    "    df = df.replace('','nan')\n",
    "\n",
    "    df[cats] = df[cats].astype(str)\n",
    "    df[nums] = df[nums].astype('float64')\n",
    "    \n",
    "    df = df.dropna(subset=['Make','Model','ModelYear','VIN'])\n",
    "    \n",
    "    value_filter = (df.VehicleType.isin(['TRUCK', 'MULTIPURPOSE PASSENGER VEHICLE (MPV)', 'PASSENGER CAR'])) & (df['BodyClass'].isin(['Pickup',\n",
    "       'Sport Utility Vehicle (SUV)/Multi-Purpose Vehicle (MPV)',\n",
    "       'Crossover Utility Vehicle (CUV)', 'Sedan/Saloon',\n",
    "       'Hatchback/Liftback/Notchback', 'Coupe', 'Convertible/Cabriolet',\n",
    "       'Minivan', 'Wagon', 'Cargo Van', 'Van']))\n",
    "       \n",
    "    df2 = df[value_filter]\n",
    "    df_bad = df[~value_filter]\n",
    "    \n",
    "    return df2, df_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10824734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "    return {}\n",
    "\n",
    "def vin_decode(vin):\n",
    "    json_data = get_json(f'https://vpic.nhtsa.dot.gov/api/vehicles/decodevinvalues/{vin}?format=json')\n",
    "    \n",
    "    if json_data['Results']:\n",
    "        df = pd.DataFrame(json_data['Results'])\n",
    "    else: \n",
    "        df = pd.DataFrame(columns=cats+nums)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85792fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "read pd.read_sql('new_listings_' + todaysdate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_output_ls1 = []\n",
    "\n",
    "for vin in listing_df[~listing_df['VIN'].isin(vin_output['VIN'])]['VIN'].unique():\n",
    "    vin_output_ls1.append(vin_decode(vin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed02cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_output = pd.concat(vin_output_ls)\n",
    "\n",
    "vin_output.to_csv('vin_output_2024_10_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_output1 = pd.concat(vin_output_ls1)\n",
    "\n",
    "vin_output1.to_csv('vin_output1_2024_10_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df[listing_df.VIN == 'WAUDT48H14K020081Y'].VIN.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aef710",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vin_df = listing_df[listing_df.VIN == 'VIN']\n",
    "test_vin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vin_df.loc[test_vin_df[test_vin_df.VIN=='WAUDT48H14K020081Y'].index,'VIN'] = 'WAUDT48H14K020081'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943954f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ls = []\n",
    "\n",
    "for vin in test_vin_df.VIN.unique():\n",
    "    df = vin_decode(vin)\n",
    "    vin2 = df.loc[0,'VIN']\n",
    "    if vin2 != vin:\n",
    "        test_vin_df.loc[test_vin_df[test_vin_df.VIN==vin].index,'VIN'] = vin2\n",
    "    test_ls.append(df)\n",
    "    \n",
    "vin_output = pd.concat(test_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12917de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_decode(listing_df, max_retries=10):\n",
    "    test_ls = []\n",
    "    remaining_vins = listing_df['VIN'].unique().tolist()  # Start with all unique VINs\n",
    "\n",
    "    while remaining_vins:\n",
    "        vin = remaining_vins.pop(0)  # Get the next VIN to process\n",
    "        retries = 0\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                df = vin_decode(vin)  # Attempt to decode the VIN\n",
    "                vin2 = df.loc[0, 'VIN']  # Extract the decoded VIN\n",
    "\n",
    "                if vin2 != vin:\n",
    "                    # Update the original DataFrame if the decoded VIN is different\n",
    "                    listing_df.loc[listing_df[listing_df.VIN == vin].index, 'VIN'] = vin2\n",
    "\n",
    "                test_ls.append(df)  # Add the DataFrame to the list\n",
    "                break  # Break the retry loop if successful\n",
    "            except requests.exceptions.Timeout:\n",
    "                retries += 1\n",
    "                print(f'Timeout occurred for {vin}. Retrying {retries}/{max_retries}...')\n",
    "                time.sleep(2)  # Optional: wait before retrying\n",
    "            except Exception as e:\n",
    "                print(f'Error occurred for {vin}: {e}')\n",
    "                break  # Exit the retry loop on other exceptions\n",
    "\n",
    "        # If all retries fail, re-add the VIN to the remaining list for later attempts\n",
    "        if retries == max_retries:\n",
    "            print(f'Failed to decode VIN {vin} after {max_retries} attempts. Will retry later.')\n",
    "            remaining_vins.append(vin)\n",
    "\n",
    "        time.sleep(1)  # Optional: Sleep between requests to avoid overwhelming the server\n",
    "\n",
    "    # Concatenate all the DataFrames collected from the successful VIN decodes\n",
    "    if test_ls:\n",
    "        vin_output = pd.concat(test_ls, ignore_index=True)\n",
    "    else:\n",
    "        vin_output = pd.DataFrame()  # Create an empty DataFrame if no successful decodes\n",
    "\n",
    "    return vin_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4b94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df.loc[10066, 'VIN'] = 'WAUDT48H14K020081'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([vin_output, vin_output1]).reset_index(drop=True)['VIN'] == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc144a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.merge(pd.concat([vin_output, vin_output1]), listing_df, on='VIN', how='left')\n",
    "f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db932c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vin_output(df):\n",
    "    df = df[df['ErrorCode'].isin(['0','1'])]\n",
    "    df = posting_date(df)\n",
    "    df = df.replace('','nan')\n",
    "\n",
    "    df[cats] = df[cats].astype(str)\n",
    "    df[nums] = df[nums].astype('float64')\n",
    "    \n",
    "    df = df.dropna(subset=['Make','Model','ModelYear','VIN'])\n",
    "    \n",
    "    value_filter = (df.VehicleType.isin(['TRUCK', 'MULTIPURPOSE PASSENGER VEHICLE (MPV)', 'PASSENGER CAR'])) & (df['BodyClass'].isin(['Pickup',\n",
    "       'Sport Utility Vehicle (SUV)/Multi-Purpose Vehicle (MPV)',\n",
    "       'Crossover Utility Vehicle (CUV)', 'Sedan/Saloon',\n",
    "       'Hatchback/Liftback/Notchback', 'Coupe', 'Convertible/Cabriolet',\n",
    "       'Minivan', 'Wagon', 'Cargo Van', 'Van']))\n",
    "       \n",
    "    df2 = df[value_filter]\n",
    "    df_bad = df[~value_filter]\n",
    "    \n",
    "    return df2, df_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e23dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_df1, df_leftovers = clean_vin_output(f_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ddaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric = f_df1[pd.to_numeric(f_df1['price'], errors='coerce').isna()]\n",
    "\n",
    "non_numeric[non_numeric.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df2 = posting_date(f_df1)\n",
    "#df3 = posting_date(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat([df3,f_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8926a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df[nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df[cats] = total_df[cats].astype(str)\n",
    "total_df[nums] = total_df[nums].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0603d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb7 = CatBoostRegressor(iterations=500, depth=11, learning_rate=0.1, loss_function='RMSE', silent=True)\n",
    "cb7.fit(total_df[cats+nums], total_df['price'], cat_features=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred7 = cb7.predict(total_df[cats+nums])\n",
    "rmse = np.sqrt(mean_squared_error(y_pred7, total_df['price']))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ae769",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb7.save_model('cb_model_latest.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(cats+nums, cb7.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['pred_2024_10_22'] = y_pred7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b506e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles1 = df_vehicles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = impute_engine_cylinders(df_vehicles1)\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b590b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_engine_cylinders(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Separate rows where EngineCylinders is 'nan' (as a string)\n",
    "    missing_cylinders = df_copy[df_copy['EngineCylinders'] == 'nan'].copy()\n",
    "\n",
    "    for idx, row in missing_cylinders.iterrows():\n",
    "        # Find rows with the same GVWR and BodyClass\n",
    "        potential_matches = df_copy[\n",
    "            (df_copy['GVWR'] == row['GVWR']) &\n",
    "            (df_copy['BodyClass'] == row['BodyClass']) &\n",
    "            (df_copy['EngineCylinders'] != 'nan')  # Ignore rows where EngineCylinders is 'nan'\n",
    "        ].copy()  # Explicitly copy to avoid SettingWithCopyWarning\n",
    "\n",
    "        if not potential_matches.empty:\n",
    "            # Initialize a total difference column with 0\n",
    "            potential_matches['total_diff'] = 0\n",
    "\n",
    "            # Calculate the absolute difference for DisplacementCC only if it is not null\n",
    "            if pd.notna(row['DisplacementCC']):\n",
    "                potential_matches['displacement_diff'] = abs(potential_matches['DisplacementCC'] - row['DisplacementCC'])\n",
    "                potential_matches['total_diff'] += potential_matches['displacement_diff'].fillna(0)\n",
    "\n",
    "            # Calculate the absolute difference for EngineHP only if it is not null\n",
    "            if pd.notna(row['EngineHP']):\n",
    "                potential_matches['enginehp_diff'] = abs(potential_matches['EngineHP'] - row['EngineHP'])\n",
    "                potential_matches['total_diff'] += potential_matches['enginehp_diff'].fillna(0)\n",
    "\n",
    "            # Get the row with the smallest total difference (or without difference if all are NaN)\n",
    "            best_match = potential_matches.loc[potential_matches['total_diff'].idxmin()]\n",
    "\n",
    "            # Impute the missing EngineCylinders with the value from the best match\n",
    "            df_copy.loc[idx, 'EngineCylinders'] = best_match['EngineCylinders']\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_csv('df_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f17762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles = model_prep(pd.read_csv('car_data_latest.csv', index_col=[0], dtype={'EngineCylinders': str}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82326099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles[df_vehicles.DisplacementCC == 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28743af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles[df_vehicles.EngineCylinders == 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'BodyClass','DisplacementCC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe130b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles['EngineCylinders'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles[df_vehicles.DisplacementCC.isnull()][['Make','Model','ModelYear', 'EngineCylinders']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_latest = total_df[[x for x in total_df.columns.to_list() if x in f_df1.columns.tolist()] + ['pred_2024_10_22']]#.to_csv('car_data_latest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4623ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_latest.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_cols = ['y_pred','error','abs_error','pred','Message', 'date_posted','displ','Year', 'description', 'county',\n",
    "'image_url', 'sizse'\n",
    "total_df.Message.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b944d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(total_df['days_since'], total_df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eee040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_more_values(gd_links, bd_links, gd_listings, bd_listings, gd_vins, reg_ref, new_table_name, engine):\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # Step 1: Remove duplicate VINs in gd_vins\n",
    "        deduplicate_gd_vins = text(f\"\"\"\n",
    "        DELETE FROM {gd_vins}\n",
    "        WHERE ctid NOT IN (\n",
    "            SELECT min(ctid)\n",
    "            FROM {gd_vins}\n",
    "            GROUP BY \"VIN\"\n",
    "        );\n",
    "        \"\"\")\n",
    "        conn.execute(deduplicate_gd_vins)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Step 2: Perform left joins and create the new table with specified columns\n",
    "        join_to_new_table = text(f\"\"\"\n",
    "        CREATE TABLE {new_table_name} AS \n",
    "        WITH combined_data AS (\n",
    "            SELECT \n",
    "                l.\"condition\", \n",
    "                l.\"drive\", \n",
    "                l.\"fuel\", \n",
    "                l.\"odometer\", \n",
    "                l.\"paint_color\", \n",
    "                l.\"title_status\", \n",
    "                l.\"transmission\", \n",
    "                l.\"type\", \n",
    "                l.\"posting_date\", \n",
    "                l.\"lat\", \n",
    "                l.\"long\", \n",
    "                l.\"geo_placename\", \n",
    "                l.\"geo_region\", \n",
    "                l.\"postingbody\", \n",
    "                l.\"title\", \n",
    "                l.\"link\", \n",
    "                v.*, \n",
    "                k.price, \n",
    "                k.location, \n",
    "                k.region_url, \n",
    "                r.state, \n",
    "                r.state_income, \n",
    "                r.region\n",
    "            FROM {gd_vins} v\n",
    "            LEFT JOIN {gd_listings} l USING (\"VIN\")\n",
    "            LEFT JOIN {gd_links} k USING (\"link\")\n",
    "            LEFT JOIN {reg_ref} r ON k.\"region_url\" = r.\"region_url\"\n",
    "        )\n",
    "        SELECT * FROM combined_data;\n",
    "        \"\"\")\n",
    "        conn.execute(join_to_new_table)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Step 3: Move rows left out of joins to bd_listings and bd_links\n",
    "        # Move rows from gd_listings to bd_listings if they are not in new_table\n",
    "        move_to_bd_listings = text(f\"\"\"\n",
    "        INSERT INTO {bd_listings}\n",
    "        SELECT * FROM {gd_listings}\n",
    "        WHERE \"VIN\" NOT IN (SELECT \"VIN\" FROM {new_table_name});\n",
    "        \n",
    "        DELETE FROM {gd_listings}\n",
    "        WHERE \"VIN\" NOT IN (SELECT \"VIN\" FROM {new_table_name});\n",
    "        \"\"\")\n",
    "        conn.execute(move_to_bd_listings)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Move rows from gd_links to bd_links if they are not in new_table\n",
    "        move_to_bd_links = text(f\"\"\"\n",
    "        INSERT INTO {bd_links}\n",
    "        SELECT * FROM {gd_links}\n",
    "        WHERE \"link\" NOT IN (SELECT \"link\" FROM {new_table_name});\n",
    "        \n",
    "        DELETE FROM {gd_links}\n",
    "        WHERE \"link\" NOT IN (SELECT \"link\" FROM {new_table_name});\n",
    "        \"\"\")\n",
    "        conn.execute(move_to_bd_links)\n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"Table {new_table_name} created with joined data; rows moved to {bd_listings} and {bd_links} if left out.\")\n",
    "\n",
    "        conn.execute(text(f\"\"\"\n",
    "            ALTER TABLE {new_table_name}\n",
    "            ADD COLUMN IF NOT EXISTS days_since INTEGER,\n",
    "            ADD COLUMN IF NOT EXISTS reference_date TEXT;\n",
    "        \"\"\"))\n",
    "\n",
    "        conn.execute(text(f\"\"\"\n",
    "            UPDATE {new_table_name}\n",
    "            SET \n",
    "                days_since = EXTRACT(DAY FROM (posting_date - DATE '2021-01-01')),\n",
    "                reference_date = '2021-01-01'\n",
    "            WHERE posting_date IS NOT NULL;\n",
    "        \"\"\"))\n",
    "        conn.commit()        \n",
    "        \n",
    "        return pd.read_sql(new_table_name, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_since']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672bc76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df1.to_csv('f_df1_2024_22_10.csv')\n",
    "df_leftovers.to_csv('leftovers_2024_22_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['ABS',\n",
    " 'Trim2',\n",
    " 'ESC',\n",
    " 'SteeringLocation',\n",
    " 'BatteryInfo',\n",
    " 'DaytimeRunningLight',\n",
    " 'PedestrianAutomaticEmergencyBraking',\n",
    " 'TransmissionStyle',\n",
    " 'WheelBaseType',\n",
    " 'Trim',\n",
    " 'ChargerLevel',\n",
    " 'AutomaticPedestrianAlertingSound',\n",
    " 'TractionControl',\n",
    " 'AirBagLocFront',\n",
    " 'Pretensioner',\n",
    " 'TransmissionSpeeds',\n",
    " 'AdaptiveDrivingBeam',\n",
    " 'Model',\n",
    " 'BlindSpotMon',\n",
    " 'EntertainmentSystem',\n",
    " 'BodyCabType',\n",
    " 'FuelTypeSecondary',\n",
    " 'LaneDepartureWarning',\n",
    " 'TPMS',\n",
    " 'Seats',\n",
    " 'FuelInjectionType',\n",
    " 'EDR',\n",
    " 'LowerBeamHeadlampLightSource',\n",
    " 'ParkAssist',\n",
    " 'AirBagLocCurtain',\n",
    " 'RearAutomaticEmergencyBraking',\n",
    " 'RearCrossTrafficAlert',\n",
    " 'SemiautomaticHeadlampBeamSwitching',\n",
    " 'CIB',\n",
    " 'AirBagLocSide',\n",
    " 'BrakeSystemDesc',\n",
    " 'KeylessIgnition',\n",
    " 'EngineConfiguration',\n",
    " 'AirBagLocKnee',\n",
    " 'RearVisibilitySystem',\n",
    " 'VehicleType',\n",
    " 'AdaptiveCruiseControl',\n",
    " 'AirBagLocSeatCushion',\n",
    " 'BlindSpotIntervention',\n",
    " 'ForwardCollisionWarning',\n",
    " 'SeatRows',\n",
    " 'BatteryType',\n",
    " 'LaneKeepSystem',\n",
    " 'GVWR',\n",
    " 'ElectrificationLevel',\n",
    " 'DynamicBrakeSupport',\n",
    " 'LaneCenteringAssistance',\n",
    " 'BedType',\n",
    " 'BrakeSystemType',\n",
    " 'Series2',\n",
    " 'CoolingType',\n",
    " 'Doors',\n",
    " 'EngineCylinders',\n",
    " 'CAN_AACN',\n",
    " 'Turbo',\n",
    " 'BodyClass',\n",
    " 'DriveType',\n",
    " 'ValveTrainDesign',\n",
    " 'FuelTypePrimary',\n",
    " 'Make',\n",
    " 'AutoReverseSystem',\n",
    " 'EVDriveUnit',\n",
    " 'Series',\n",
    " 'SeatBeltsAll',\n",
    " 'PlantCity',\n",
    " 'PlantCountry',\n",
    " 'PlantState',\n",
    " 'Note',\n",
    " 'OtherEngineInfo',\n",
    " 'GVWR_to',\n",
    " 'EngineModel',\n",
    " 'DestinationMarket',\n",
    " 'ActiveSafetySysNote',\n",
    " 'state',\n",
    " 'region',\n",
    " 'condition',\n",
    "'paint_color']\n",
    "\n",
    "nums =  ['ModelYear',\n",
    " 'WheelSizeRear',\n",
    " 'BasePrice',\n",
    " 'WheelSizeFront',\n",
    " 'CurbWeightLB',\n",
    " 'WheelBaseShort',\n",
    " 'WheelBaseLong',\n",
    " 'BatteryPacks',\n",
    " 'SAEAutomationLevel',\n",
    " 'odometer',\n",
    " 'EngineHP',\n",
    " 'TopSpeedMPH',\n",
    " 'TrackWidth',\n",
    " 'ChargerPowerKW',\n",
    " 'EngineKW',\n",
    " 'EngineHP_to',\n",
    " 'BatteryKWh',\n",
    " 'BedLengthIN',\n",
    " 'BatteryV',\n",
    " 'DisplacementCC',\n",
    " 'Wheels',\n",
    " 'Windows',\n",
    " 'days_since',\n",
    " 'state_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posting_date'] = pd.to_datetime(df['posting_date'])\n",
    "df['posting_date'] - pd.to_timedelta(df['date_posted'], unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posting_date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posting_date'] = pd.to_datetime(df['posting_date'])\n",
    "\n",
    "# Define the reference date\n",
    "reference_date = pd.to_datetime('2021-01-01')\n",
    "\n",
    "# Create 'days_since' column by subtracting the reference date from posting_date and converting to days\n",
    "df['days_since'] = (df['posting_date'] - reference_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date_posted.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f005539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posting_date(df):\n",
    "    #df['posting_date'] = pd.to_datetime(df['posting_date']).dt.tz_localize(None)\n",
    "    df['posting_date'] = pd.to_datetime(df['posting_date'], utc=True).dt.tz_localize(None)\n",
    "    reference_date = pd.to_datetime('2021-01-01')\n",
    "    df['days_since'] = (df['posting_date'] - reference_date).dt.days\n",
    "    df['reference_date'] = reference_date\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = posting_date(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d27296",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = posting_date(f_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = pd.to_datetime('2021-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_vehicle_info_and_predict(df, folder):\n",
    "    # Step 1: Fill in extra information\n",
    "    df['odometer'] = 100000 \n",
    "    df['paint_color'] = 'white'\n",
    "    df['condition'] = 'good'\n",
    "    df['state_income'] = 59802\n",
    "    df['state'] = 'tx'\n",
    "    df['region'] = 'dallas / fort worth'\n",
    "\n",
    "    \n",
    "    \n",
    "    models = os.listdir(folder)    \n",
    "    model_dates = []\n",
    "\n",
    "    # Step 3: Run predictions with each model\n",
    "    predicted_prices = []\n",
    "    for model in models:\n",
    "        cbm = CatBoostRegressor()\n",
    "        cbm.load_model(model)  # Assume you have a function to load your model\n",
    "        y_pred = cbm.predict(df[cats + nums])  # Replace 'cats' and 'nums' with your actual feature lists\n",
    "        predicted_prices.append(y_pred)  # Append the first prediction\n",
    "        \n",
    "        model_dates.append(remove_prefix_suffix(model, prefix='cb_model_', suffix='.cbm'))\n",
    "\n",
    "    # Step 4: Plot the results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(model_dates, predicted_prices, marker='o')\n",
    "    plt.title('Predicted Prices Over Time: ' + df['Make'] +' ' + df['Model'] + ' ' + df['DisplacementCC'].astype(str))\n",
    "    plt.xlabel('Model Date')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model_folder = 'cb_models'\n",
    "row = df.loc[100]\n",
    "fill_vehicle_info_and_predict(row, model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547e843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1cd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix_suffix(filename, prefix='cb_model_', suffix='.cbm'):\n",
    "    return filename[len(prefix):-len(suffix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdaf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder\n",
    "model_files = [f for f in os.listdir(model_folder) if f.startswith('cb_model_')]\n",
    "model_dates = sorted([f.split('_')[2] for f in model_files])  # Extract dates\n",
    "model_dates = [f'cb_model_{date}' for date in model_dates]  # Add prefix back\n",
    "model_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ceec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.getcwd(), 'cb_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ordered_model_dates(os.path.join(os.getcwd(), 'cb_models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f44062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = os.path.join(os.getcwd(), 'cb_models')\n",
    "#folder\n",
    "get_ordered_model_dates(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = folder + '\\\\cb_model_2024_10_29.cbm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordered_model_dates(folder_path, prefix='cb_model_', suffix='.cbm'):\n",
    "    date_suffixes = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the filename starts with the prefix and ends with the suffix\n",
    "        if filename.startswith(prefix) and filename.endswith(suffix):\n",
    "            date_str = filename[len(prefix):-len(suffix)]  # Extract the date part\n",
    "            print(date_str)  # Optional: Print the extracted date string\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, '%Y_%m_%d')  # Convert to datetime object\n",
    "                date_suffixes.append(date_obj)  # Append to the list\n",
    "            except ValueError:\n",
    "                print(f\"Date format error in filename: {filename}\")  # Handle date format errors\n",
    "                \n",
    "    ordered_dates = sorted(date_suffixes)  # Sort the dates\n",
    "    \n",
    "    # Return the sorted dates in the original string format\n",
    "    return [date.strftime('%Y_%m_%d') for date in ordered_dates]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbm.predict(df.loc[100][cats+nums])#, cat_features=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e06b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ed478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_sql('car_data', engine)\n",
    "df = model_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2946d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reference_date'] = reference_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261348bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df2.copy()\n",
    "\n",
    "df['reference_date'] = df['posting_date'] - pd.to_timedelta(df['date_posted'], unit='d')\n",
    "df['reference_date']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
